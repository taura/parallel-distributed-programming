{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-001",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#  Cost of Data Access (Caches and Memory Performance)",
        "\n",
        "# 1. Introduction",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-002",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 2. Compilers",
        "\n",
        "## 2-1. Set up NVIDIA CUDA and HPC SDK",
        "\n",
        "Execute this before you use NVIDIA HPC SDK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-003",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.9/compilers/bin:$PATH\n",
        "export PATH=/usr/local/cuda/bin:$PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-004",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Check if it works\n",
        "  * make sure the full path of nvcc is shown as `/usr/local/...`, not `/opt/nvidia/...`\n",
        "* We do not recommend nvc/nvc++ for this exercise, but you might give them a try if you like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-005",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "which nvcc\n",
        "which nvc\n",
        "which nvc++\n",
        "nvcc --version\n",
        "nvc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-006",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2-2. Set up LLVM",
        "\n",
        "Execute this before you use LLVM\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-007",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "export PATH=/home/share/llvm/bin:$PATH\n",
        "export LD_LIBRARY_PATH=/home/share/llvm/lib:/home/share/llvm/lib/x86_64-unknown-linux-gnu:$LD_LIBRARY_PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-008",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Check if it works (check if full paths of gcc/g++ are shown)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-009",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "which clang\n",
        "which clang++"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-010",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2-3. GCC",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-011",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Check if it works (check if full paths of nvc/nvc++ are shown)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-012",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "which gcc\n",
        "which g++"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-013",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 3. Measuring Latency",
        "\n",
        "* We do a simple experiment to measure the _latency_ of data access when the data comes from various levels of caches\n",
        "* We want to execute many ($n$) load instructions each of which is _dependent_ on the previous load instruction, measure the execution time ($T$), and divide it by $n$ (to get $T/n$)\n",
        "* To make a load instruction dependent on the previous load instruction, we determine which address it accesses based on the result of the previous load instruction, like this\n",
        "```\n",
        "k = 0;\n",
        "for (i = 0; i < n; i++) {\n",
        "  k = a[k];\n",
        "}\n",
        "```\n",
        "* A similar access behavior happens when the processor _chases pointers_ like this\n",
        "```\n",
        "p = ...;\n",
        "for (i = 0; i < n; i++) {\n",
        "  p = p->next;\n",
        "}\n",
        "```\n",
        "so we call this kind of code _pointer chasing_ code, although we do not explicitly use pointers ($k$ serves as a \"pseudo pointer\" that specifies the next element that should be accessed)\n",
        "* We change the size of array $a$ and make sure the above loop repeatedly touches every element of $a$\n",
        "* Here is an example of $a$ (with 16 elements) and (part of) the resulting access order (`a[0] -> a[3] -> a[14] -> a[10] -> a[7] -> a[15] -> a[1] -> ... -> a[4] -> a[0] -> ..`)\n",
        "* confirm that the resulting chain comes back to `a[0]` after accessing _all_ 16 elements of the array $a$\n",
        "\n",
        "<img src=\"svg/latency_measurement_L1.svg\" />\n",
        "\n",
        "* We also make sure the resulting access order is essentially random, to avoid the effect of prefetching or any smartness the processor might implement to run the above loop faster than an iteration / latency of the load instruction.\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-014",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* We use only a single thread for now,\n",
        "* Although it is meant to be a single-thread experiment, we still use OpenMP so that it can run on GPUs too (with a single source code)\n",
        "* For readability, we split the program into two files\n",
        "* Here is the main function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-015",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile main.cc\n",
        "#include <assert.h>\n",
        "#include <err.h>\n",
        "#include <getopt.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <time.h>\n",
        "#if __NVCOMPILER                // NVIDIA nvc++\n",
        "#else  // Clang\n",
        "#define __host__\n",
        "#define __device__\n",
        "#define __global__\n",
        "#endif\n",
        "\n",
        "/* get current time in nanosecond */\n",
        "double cur_time() {\n",
        "  struct timespec ts[1];\n",
        "  int ok = clock_gettime(CLOCK_REALTIME, ts);\n",
        "  if (ok == -1) err(1, \"clock_gettime\");\n",
        "  return ts->tv_sec + ts->tv_nsec * 1e-9;\n",
        "}\n",
        "\n",
        "/* random number generator */\n",
        "struct prng {\n",
        "  long sd;\n",
        "  void seed(long sd) {\n",
        "    this->sd = sd;\n",
        "  }\n",
        "  long gen_randint() {\n",
        "    long a = 0x5DEECE66Dull;\n",
        "    long c = 0xB;\n",
        "    sd = (a * sd + c) & ((1L << 48) - 1);\n",
        "    long y = sd >> 17;\n",
        "    return y;\n",
        "  }\n",
        "};\n",
        "\n",
        "/* allocate on the device the main thing will be run */\n",
        "template<typename T>\n",
        "T * alloc_dev(size_t n_elems) {\n",
        "  size_t sz = sizeof(T) * n_elems;\n",
        "  T * b;\n",
        "  if (-1 == posix_memalign((void **)&b, 4096, sz)) err(1, \"posix_memalign\");\n",
        "  return b;\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "void dealloc_dev(T * b) {\n",
        "  free((void *)b);\n",
        "}\n",
        "\n",
        "/* swap a[i] and a[j] */\n",
        "void swap(long * a, long i, long j) {\n",
        "  long ai = a[i];\n",
        "  long aj = a[j];\n",
        "  a[i] = aj;\n",
        "  a[j] = ai;\n",
        "}\n",
        "\n",
        "/* shuffle seq = [0,1,2,...,n_cycles*len_cycle-1];\n",
        "   make sure\n",
        "   (1) coalese_size consecutive elements are sequential.\n",
        "   (2) seq[0:n_cycles] = [0,1,...,n_cycles] */\n",
        "void shuffle(long * seq, long coalese_size,\n",
        "             long n_cycles, long len_cycle, long seed) {\n",
        "  assert(n_cycles % coalese_size == 0);\n",
        "  long m = n_cycles * len_cycle;\n",
        "  for (long i = 0; i < m; i++) {\n",
        "    seq[i] = i;\n",
        "  }\n",
        "  if (seed >= 0) {\n",
        "    prng rg;\n",
        "    rg.seed(seed);\n",
        "    long n_blocks = m / coalese_size;\n",
        "    for (long i = n_cycles / coalese_size; i < n_blocks; i++) {\n",
        "      long j = rg.gen_randint() % (n_blocks - i);\n",
        "      for (long k = 0; k < coalese_size; k++) {\n",
        "        swap(seq, i * coalese_size + k, (i + j) * coalese_size + k);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "/* set a[k] to the next element to access */\n",
        "__host__ __device__\n",
        "void make_cycle(long * a, long * seq,\n",
        "                long start_idx, long n_cycles, long len_cycle) {\n",
        "  // a cycle starting from seq[idx] :\n",
        "  // a[seq[idx]] -> a[seq[idx+n_cycles]] -> a[seq[idx+2*n_cycles]] -> ..\n",
        "  long m = n_cycles * len_cycle;\n",
        "  for (long i = 0; i < len_cycle; i++) {\n",
        "    long cur  = seq[ start_idx +      i  * n_cycles];\n",
        "    long next = seq[(start_idx + (i + 1) * n_cycles) % m];\n",
        "    a[cur] = next;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "void make_cycles(long * a, long * seq, long m,\n",
        "                 long n_cycles, long len_cycle, \n",
        "                 long n_teams, long n_threads_per_team) {\n",
        "#pragma omp target teams distribute parallel for num_teams(n_teams) num_threads(n_threads_per_team) map(tofrom: a[0:m], seq[0:m])\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    make_cycle(a, seq, idx, n_cycles, len_cycle);\n",
        "  }\n",
        "}\n",
        "\n",
        "/* defined in a separate file (latency.cc or latency_ilp.cc) */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx);\n",
        "\n",
        "struct opts {\n",
        "  /* number of elements */\n",
        "  long n_elements;\n",
        "  /* minimum number of scans */\n",
        "  double min_scans;\n",
        "  /* minimum number of accesses */\n",
        "  long min_accesses;\n",
        "  /* number of consecutive elements guaranteed to be contiguous */\n",
        "  long coalese_size;\n",
        "  long n_cycles;\n",
        "  long n_conc_cycles;\n",
        "  long seed;\n",
        "  opts() {\n",
        "    n_elements = 1L << 24;\n",
        "    min_scans = 5.3;\n",
        "    min_accesses = (1 << 20);\n",
        "    coalese_size = 1;\n",
        "    n_cycles = 1;\n",
        "    n_conc_cycles = 1;\n",
        "    seed = 123456789012345L;\n",
        "  }\n",
        "};\n",
        "\n",
        "void usage(char * prog) {\n",
        "  opts o;\n",
        "  fprintf(stderr, \"usage:\\n\");\n",
        "  fprintf(stderr, \"  %s [options]\\n\", prog);\n",
        "  fprintf(stderr, \"options:\\n\");\n",
        "  fprintf(stderr, \"  -m,--n-elements (%ld)\\n\", o.n_elements);\n",
        "  fprintf(stderr, \"  --min-scans N (%.3f)\\n\", o.min_scans);\n",
        "  fprintf(stderr, \"  --min-accesses N (%ld)\\n\", o.min_accesses);\n",
        "  fprintf(stderr, \"  -c,--coalese-size N (%ld)\\n\", o.coalese_size);\n",
        "  fprintf(stderr, \"  --n-cycles N (%ld)\\n\", o.n_cycles);\n",
        "  fprintf(stderr, \"  --seed N (%ld)\\n\", o.seed);\n",
        "}\n",
        "\n",
        "opts parse_opts(int argc, char ** argv) {\n",
        "  static struct option long_options[] = {\n",
        "    {\"n-elements\",          required_argument, 0, 'm' },\n",
        "    {\"min-scans\",           required_argument, 0, 0 },\n",
        "    {\"min-accesses\",        required_argument, 0, 0 },\n",
        "    {\"coalese-size\",        required_argument, 0, 0 },\n",
        "    {\"n-cycles\",            required_argument, 0, 0 },\n",
        "    {\"n-conc-cycles\",       required_argument, 0, 0 },\n",
        "    {\"seed\",                required_argument, 0, 0 },\n",
        "    {0,         0,                 0,  0 }\n",
        "  };\n",
        "  opts o;\n",
        "  while (1) {\n",
        "    int option_index = 0;\n",
        "    int c = getopt_long(argc, argv, \"m:c:\",\n",
        "\t\t\tlong_options, &option_index);\n",
        "    if (c == -1) break;\n",
        "\n",
        "    switch (c) {\n",
        "    case 'm':\n",
        "      o.n_elements = atol(optarg);\n",
        "      break;\n",
        "    case 0:\n",
        "      {\n",
        "        const char * opt_name = long_options[option_index].name;\n",
        "        if (strcmp(opt_name, \"seed\") == 0) {\n",
        "          o.seed = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"min-scans\") == 0) {\n",
        "          o.min_scans = atof(optarg);\n",
        "        } else if (strcmp(opt_name, \"min-accesses\") == 0) {\n",
        "          o.min_accesses = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"coalese-size\") == 0) {\n",
        "          o.coalese_size = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"n-cycles\") == 0) {\n",
        "          o.n_cycles = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"n-conc-cycles\") == 0) {\n",
        "          o.n_conc_cycles = atol(optarg);\n",
        "        } else {\n",
        "          usage(argv[0]);\n",
        "          exit(1);\n",
        "        }\n",
        "        break;\n",
        "      }\n",
        "    default:\n",
        "      usage(argv[0]);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "  return o;\n",
        "}\n",
        "\n",
        "long getenv_long(const char * s) {\n",
        "  char * vs = getenv(s);\n",
        "  if (!vs) {\n",
        "    fprintf(stderr, \"set environment variable %s\\n\", s);\n",
        "    exit(0);\n",
        "  }\n",
        "  return atol(vs);\n",
        "}\n",
        "\n",
        "typedef long longv;\n",
        "\n",
        "int main(int argc, char ** argv) {\n",
        "  const long L = sizeof(longv) / sizeof(long);\n",
        "  long n_teams = getenv_long(\"OMP_NUM_TEAMS\");\n",
        "  long n_threads_per_team = getenv_long(\"OMP_NUM_THREADS\");\n",
        "  opts opt = parse_opts(argc, argv);\n",
        "  long m = opt.n_elements;\n",
        "  long n_cycles = opt.n_cycles;\n",
        "  long coalese_size = opt.coalese_size;\n",
        "  assert(n_cycles % coalese_size == 0);\n",
        "  assert(coalese_size % L == 0);\n",
        "  long len_cycle = (m + n_cycles - 1) / n_cycles;\n",
        "  if (m % n_cycles) {\n",
        "    fprintf(stderr,\n",
        "            \"WARNING : m (%ld) not divisible by n_cycles (%ld),\"\n",
        "            \" rounded up to %ld\\n\",\n",
        "            m, n_cycles, len_cycle * n_cycles);\n",
        "    m = len_cycle * n_cycles;\n",
        "  }\n",
        "  printf(\"n_elements : %ld\\n\", m);\n",
        "  size_t sz = sizeof(long) * m;\n",
        "  printf(\"sz : %ld bytes\\n\", sz);\n",
        "  printf(\"n_cycles : %ld\\n\", n_cycles);\n",
        "  printf(\"len_cycle : %ld\\n\", len_cycle);\n",
        "  double s = opt.min_scans;\n",
        "  long n = len_cycle * s;\n",
        "  if (n * n_cycles < opt.min_accesses) {\n",
        "    n = (opt.min_accesses + n_cycles - 1) / n_cycles;\n",
        "  }\n",
        "  printf(\"n_accesses_per_cycle : %ld\\n\", n);\n",
        "  printf(\"total_accesses : %ld\\n\", n * n_cycles);\n",
        "  long n_conc_cycles = opt.n_conc_cycles;\n",
        "  printf(\"n_conc_cycles : %ld\\n\", n_conc_cycles);\n",
        "  assert(n_cycles % n_conc_cycles == 0);\n",
        "  printf(\"coalese_size : %ld\\n\", coalese_size);\n",
        "  printf(\"seed : %ld\\n\", opt.seed);\n",
        "\n",
        "  long * seq = alloc_dev<long>(m);\n",
        "  shuffle(seq, coalese_size, n_cycles, len_cycle, opt.seed);\n",
        "\n",
        "  long * a = alloc_dev<long>(m);\n",
        "  double t0 = cur_time();\n",
        "  make_cycles(a, seq, m, n_cycles, len_cycle, \n",
        "              n_teams, n_threads_per_team);\n",
        "  double t1 = cur_time();\n",
        "  double dt0 = t1 - t0;\n",
        "  printf(\"make_cycles_total : %f sec\\n\", dt0);\n",
        "  printf(\"make_cycles_per_elem : %.1f nsec\\n\", 1.0e9 * dt0 / m);\n",
        "  long * end = alloc_dev<long>(n_cycles);\n",
        "  int * thread_idx = alloc_dev<int>(n_cycles);\n",
        "  double t2 = cur_time();\n",
        "  cycles(a, m, n, end, n_cycles, n_conc_cycles,\n",
        "         n_teams, n_threads_per_team, thread_idx);\n",
        "  double t3 = cur_time();\n",
        "  double dt1 = t3 - t2;\n",
        "  long bytes = sizeof(long) * n * n_cycles;\n",
        "  double bw = bytes / dt1;\n",
        "  printf(\"bytes accessed : %ld bytes\\n\", bytes);\n",
        "  printf(\"time_total : %f sec\\n\", dt1);\n",
        "  printf(\"time_per_access : %.1f nsec/access\\n\", 1.0e9 * dt1 / (n * n_cycles));\n",
        "  printf(\"bw : %.3f GB/sec\\n\", bw * 1.e-9);\n",
        "  printf(\"checking results ... \"); fflush(stdout);\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    assert(end[idx] == seq[(idx + n * n_cycles) % m]);\n",
        "  }\n",
        "  printf(\"OK\\n\");\n",
        "#if 0\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    printf(\"idx = %ld by thread %d\\n\", idx, thread_idx[idx]);\n",
        "  }\n",
        "#endif\n",
        "  dealloc_dev(seq);\n",
        "  dealloc_dev(end);\n",
        "  dealloc_dev(a);\n",
        "  return 0;\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-016",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Here is the core part of the program that accesses the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-017",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile latency.cc\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#if __NVCOMPILER                // NVIDIA nvc++\n",
        "#include <nv/target>\n",
        "__device__ int get_thread_index() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "  } else {\n",
        "    return omp_get_thread_num();\n",
        "  }\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "  } else {\n",
        "    return omp_get_num_threads();\n",
        "  }\n",
        "}\n",
        "#else  // Clang\n",
        "#define __host__\n",
        "#define __device__\n",
        "#define __global__\n",
        "__device__ int get_thread_index() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "#else\n",
        "    return omp_get_thread_num();\n",
        "#endif\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "#else\n",
        "    return omp_get_num_threads();\n",
        "#endif\n",
        "}\n",
        "#endif\n",
        "\n",
        "/* starting from cell &a[idx], chase ->next ptr n times\n",
        "   and put where it ends in end[idx] */\n",
        "__host__ __device__\n",
        "void cycle(long * a, long idx, long n, long * end, int * thread_idx) {\n",
        "  long k = idx;\n",
        "  asm volatile(\"// ========== loop begins ========== \");\n",
        "#pragma unroll(8)\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    k = a[k];\n",
        "  }\n",
        "  asm volatile(\"// ---------- loop ends ---------- \");\n",
        "  end[idx] = k;\n",
        "  thread_idx[idx] = k;\n",
        "}\n",
        "\n",
        "\n",
        "/* a is an array of m cells;\n",
        "   starting from &a[idx] for each idx in [0:n_cycles],\n",
        "   chase ->next ptr n times and put where it ends in end[idx] */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx) {\n",
        "#pragma omp target teams distribute parallel for num_teams(n_teams) num_threads(n_threads_per_team) map(tofrom: a[0:m], end[0:n_cycles], thread_idx[0:n_cycles])\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    cycle(a, idx, n, end, thread_idx);\n",
        "  }\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-018",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Compile them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-001",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "clang++ -DDBG=0 -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 -o latency latency.cc main.cc\n",
        "#nvc++   -Wall -mavx512f -mfma -mp=gpu -cuda -o latency latency.cc main.cc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-019",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* run it on a CPU with a single thread (remember `OMP_TARGET_OFFLOAD=DISABLED` disables GPU execution)\n",
        "* let's run it with $m = 2^{24}$ elements $= 8 \\times m = $ 128MB, sufficiently above its last level cache (57MB)\n",
        "* the parameter $n$ below specifies how many accesses we perform (`n` below)\n",
        "```\n",
        "k = 0;\n",
        "for (i = 0; i < n; i++) {\n",
        "  k = a[k];\n",
        "}\n",
        "```\n",
        "\n",
        "* the following command will take something like 15 seconds (be patient)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-002",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "# single thread execution on CPU\n",
        "# most data accesses will miss all caches\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "./latency --n-elements ${m} --min-accesses ${n}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-020",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* look at the number shown as\n",
        "```\n",
        "latency_per_elem : XYZ nsec/elem\n",
        "```\n",
        "which gives you the latecy imposed by _main memory_ access (when the accesses misses caches at any level)\n",
        "\n",
        "* observe the latency to main memory is very large (e.g., on 2.4 GHz processor, 1 nanosecond = 2.4 cycles, thus 80 nanoseconds is as large as 200 cycles) compared to typical latency of simple arithmetic (a few cycles)\n",
        "\n",
        "* now look at the latency of L1 (faster/smalest level) cache, buy making $a$ smaller than the L1 cache size (64KB)\n",
        "\n",
        "* we set $m = 2^{12}$ so that $a$ occupies 32KB, sufficiently smaller than L1 cache\n",
        "\n",
        "* note that we set $n$ to the same value with before, so this program executes exactly the same number of iterations, with the only difference being how large area the array $a$ spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-003",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "# single thread execution on CPU\n",
        "# most data accesses will hit L1 cache\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 12))\n",
        "export n=$((1 << 27))\n",
        "./latency --n-elements ${m} --min-accesses ${n}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-021",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* now run the same program on a GPU, again with a single (CUDA) thread (remember `OMP_TARGET_OFFLOAD=MANDATORY` makes sure the target region runs on GPU)\n",
        "* all other parameters are set equal to CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-004",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "# single thread execution on GPU\n",
        "# most data accesses will miss all caches\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "./latency --n-elements ${m} --min-accesses ${n}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-022",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* observe the latency difference between CPU and GPU\n",
        "* GPU has a few times larger latency to the main memory\n",
        "\n",
        "* let's see what happens for array $a$ smaller than the L1 cache (192KB)\n",
        "* to make a comarison to CPU, we set $m$ to the same value as the CPU experiment ($2^{12}$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-005",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "# single thread execution on GPU\n",
        "# most data accesses will hit L1 cache\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 12))\n",
        "export n=$((1 << 27))\n",
        "./latency --n-elements ${m} --min-accesses ${n}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-023",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* it is interesting to see the huge difference in L1 cache latency between CPU and GPU\n",
        "* GPU imposes several dezens of nanoseconds even when an access hits the fastest cache, whereas the L1 latency of CPU caches is as small as a few nanoseconds (a few cycles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-024",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 4. Plotting the Latencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-025",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* let's see how the latency is affected by the cache level data are coming from\n",
        "* to see this, we plot the relationship between the size of $a$ and the latency per access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-006",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export min_m=1000\n",
        "export max_m=$((1 << 25))\n",
        "export n=$((1 << 27))\n",
        "\n",
        "m=${min_m}\n",
        "while [ ${m} -lt ${max_m} ]; do\n",
        "    echo \"==== m=${m} ====\"\n",
        "    ./latency --n-elements ${m} --min-accesses ${n}\n",
        "    m=$((m * 5 / 4))\n",
        "done | tee cpu.txt\n",
        "echo \"done\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-026",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_latency([\"cpu.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-007",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export min_m=1000\n",
        "export max_m=$((1 << 25))\n",
        "export n=$((1 << 27))\n",
        "\n",
        "m=${min_m}\n",
        "while [ ${m} -lt ${max_m} ]; do\n",
        "    echo \"==== m=${m} ====\"\n",
        "    ./latency --n-elements ${m} --min-accesses ${n}\n",
        "    m=$((m * 5 / 4))\n",
        "done | tee gpu.txt\n",
        "echo \"done\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-027",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_latency([\"gpu.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-028",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Compare the CPU and the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-029",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_latency([\"cpu.txt\", \"gpu.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-030",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 5. Increasing the Bandwidth",
        "\n",
        "* The bandwidth reported above is\n",
        "\n",
        "$$ \\frac{\\mbox{sizeof(long)}}{\\mbox{latency per element}} $$\n",
        "\n",
        "and it is essentially the reciprocal (inverse) of the latency\n",
        "\n",
        "* When we look at the values when $a$ is much larger than the last level cache (so data are all coming from the main memory), the observed value is very small (e.g., $\\approx 0.08$ GB/sec on CPU and $\\approx 0.02$ GB/sec on GPU)\n",
        "* They are much smaller than the advertised hardware bandwidth ($>$ 50 GB/sec for the CPU we are using and $\\approx$ 1.5 TB/sec for the A100 GPU we are using)\n",
        "* Just as we cannot reduce the latency of arithmetic, there is no way to reduce the latency between the main memory and the processor\n",
        "* We can only increase the _bandwidth_ (the amount of data we can move per unit time) by _increasing the parallelism_\n",
        "\n",
        "* There are multiple ways to do that\n",
        "  * On CPUs, it is essential to exploit instruction level parallelism _in a single thread_, which can be done by performing several loops like this for different regions of $a$ (different `start` index below)\n",
        "```\n",
        "k = start;\n",
        "for (i = 0; i < n; i++) {\n",
        "  k = a[k];\n",
        "}\n",
        "```\n",
        "  * On GPUs, principle is the same, but parallelism can be easily and most naturally extracted by having many CUDA threads perfoming a loop like the above\n",
        "\n",
        "* Either way, we make multiple chains of pointers, each of which covers a disjoint region of the entire array\n",
        "* Here is a simple example having _two_ 8-element chains of pointers\n",
        "* Confirm that the chain starting from $a[1]$ makes another 8-element chain (a[1], a[8], a[9], ...)\n",
        "\n",
        "<img src=\"svg/latency_measurement_L2.svg\" />\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-031",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-1. Traversing Multiple Pointer Chains by A Single Thread",
        "\n",
        "* We can chase two chains simultaneously by essentially doing something like this\n",
        "```\n",
        "k0 = 0;\n",
        "k1 = 1;\n",
        "for (i = 0; i < n; i++) {\n",
        "  k0 = a[k0];\n",
        "  k1 = a[k1];\n",
        "}\n",
        "```\n",
        "\n",
        "* The code generalizes this idea so that we can chase an arbitrary number of ($C$) chains simultaneously"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-032",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile latency_ilp.cc\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#if __NVCOMPILER                // NVIDIA nvc++\n",
        "#include <nv/target>\n",
        "__device__ int get_thread_index() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "  } else {\n",
        "    return omp_get_thread_num();\n",
        "  }\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "  } else {\n",
        "    return omp_get_num_threads();\n",
        "  }\n",
        "}\n",
        "#else  // Clang\n",
        "#define __host__\n",
        "#define __device__\n",
        "#define __global__\n",
        "__device__ int get_thread_index() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "#else\n",
        "    return omp_get_thread_num();\n",
        "#endif\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "#else\n",
        "    return omp_get_num_threads();\n",
        "#endif\n",
        "}\n",
        "#endif\n",
        "\n",
        "void cycle_conc(long * a, long idx, long C, long n, long * end, int * thread_idx) {\n",
        "  long k[C];\n",
        "  /* track only every L elements */\n",
        "  for (long c = 0; c < C; c++) {\n",
        "    k[c] = idx + c;\n",
        "  }\n",
        "  asm volatile(\"// ========== loop begins ========== \");\n",
        "#pragma unroll(8)\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    for (long c = 0; c < C; c++) {\n",
        "      k[c] = a[k[c]];\n",
        "    }\n",
        "  }\n",
        "  asm volatile(\"// ---------- loop ends ---------- \");\n",
        "  for (long c = 0; c < C; c++) {\n",
        "    end[idx + c] = k[c];\n",
        "    thread_idx[idx + c] = get_thread_index();\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "/* a is an array of m cells;\n",
        "   starting from &a[idx] for each idx in [0:n_cycles],\n",
        "   chase ->next ptr n times and put where it ends in end[idx] */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx) {\n",
        "#pragma omp target teams distribute parallel for num_teams(n_teams) num_threads(n_threads_per_team) map(tofrom: a[0:m], end[0:n_cycles], thread_idx[0:n_cycles])\n",
        "  for (long idx = 0; idx < n_cycles; idx += n_conc_cycles) {\n",
        "    cycle_conc(a, idx, n_conc_cycles, n, end, thread_idx);\n",
        "  }\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-033",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* This code uses a variable-length array to have a number of ($C$) variables which is determined by command line\n",
        "* It is not supported by `nvc++`, \n",
        "* `clang++` supports this, but only on CPUs\n",
        "* Here is the trick to avoid variable length arrays by using templates up to a constant number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-034",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile latency_ilp_c.cc\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#if __NVCOMPILER                // NVIDIA nvc++\n",
        "#include <nv/target>\n",
        "__device__ int get_thread_index() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "  } else {\n",
        "    return omp_get_thread_num();\n",
        "  }\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "  if target(nv::target::is_device) {\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "  } else {\n",
        "    return omp_get_num_threads();\n",
        "  }\n",
        "}\n",
        "#else  // Clang\n",
        "#define __host__\n",
        "#define __device__\n",
        "#define __global__\n",
        "__device__ int get_thread_index() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int thread_idx;\n",
        "    unsigned int block_idx;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%ctaid.x;\" : \"=r\"(block_idx));\n",
        "    asm volatile (\"mov.u32 %0, %%tid.x;\"   : \"=r\"(thread_idx));\n",
        "    int global_idx = thread_idx + block_idx * block_dim;\n",
        "    return global_idx;\n",
        "#else\n",
        "    return omp_get_thread_num();\n",
        "#endif\n",
        "}\n",
        "__device__ int get_n_threads() {\n",
        "#if __CUDA_ARCH__\n",
        "    unsigned int grid_dim;\n",
        "    unsigned int block_dim;\n",
        "    asm volatile (\"mov.u32 %0, %%ntid.x;\"  : \"=r\"(block_dim));\n",
        "    asm volatile (\"mov.u32 %0, %%nctaid.x;\" : \"=r\"(grid_dim));\n",
        "    return grid_dim * block_dim;\n",
        "#else\n",
        "    return omp_get_num_threads();\n",
        "#endif\n",
        "}\n",
        "#endif\n",
        "\n",
        "template<long C>\n",
        "void cycle_conc_t(long * a, long idx, long n, long * end, int * thread_idx) {\n",
        "  long k[C];\n",
        "  for (long c = 0; c < C; c++) {\n",
        "    k[c] = idx + c;\n",
        "  }\n",
        "  asm volatile(\"// ========== loop begins C = %0 ========== \" : : \"i\" (C));\n",
        "#pragma unroll(8)\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    for (long c = 0; c < C; c++) {\n",
        "      k[c] = a[k[c]];\n",
        "    }\n",
        "  }\n",
        "  asm volatile(\"// ---------- loop ends C = %0 ---------- \" : : \"i\" (C));\n",
        "  for (long c = 0; c < C; c++) {\n",
        "    end[idx + c] = k[c];\n",
        "    thread_idx[idx + c] = get_thread_index();\n",
        "  }\n",
        "}\n",
        "\n",
        "void cycle_conc(long * a, long idx, long C, long n, long * end, int * thread_idx) {\n",
        "  const long max_const_c = 12;\n",
        "  long c;\n",
        "  for (c = 0; c + max_const_c <= C; c += max_const_c) {\n",
        "    cycle_conc_t<max_const_c>(a, idx + c, n, end, thread_idx + c);\n",
        "  }\n",
        "  switch (C - c) {\n",
        "  case 0:\n",
        "    break;\n",
        "  case 1:\n",
        "    cycle_conc_t<1>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 2:\n",
        "    cycle_conc_t<2>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 3:\n",
        "    cycle_conc_t<3>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 4:\n",
        "    cycle_conc_t<4>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 5:\n",
        "    cycle_conc_t<5>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 6:\n",
        "    cycle_conc_t<6>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 7:\n",
        "    cycle_conc_t<7>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 8:\n",
        "    cycle_conc_t<8>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 9:\n",
        "    cycle_conc_t<9>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 10:\n",
        "    cycle_conc_t<10>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  case 11:\n",
        "    cycle_conc_t<11>(a, idx + c, n, end, thread_idx + c);\n",
        "    break;\n",
        "  default:\n",
        "    assert(C < max_const_c);\n",
        "    break;\n",
        "  }\n",
        "}\n",
        "  \n",
        "\n",
        "\n",
        "/* a is an array of m cells;\n",
        "   starting from &a[idx] for each idx in [0:n_cycles],\n",
        "   chase ->next ptr n times and put where it ends in end[idx] */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx) {\n",
        "#pragma omp target teams distribute parallel for num_teams(n_teams) num_threads(n_threads_per_team) map(tofrom: a[0:m], end[0:n_cycles], thread_idx[0:n_cycles])\n",
        "  for (long idx = 0; idx < n_cycles; idx += n_conc_cycles) {\n",
        "    cycle_conc(a, idx, n_conc_cycles, n, end, thread_idx);\n",
        "  }\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-008",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "clang++ -DDBG=0 -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 -o latency_ilp_c latency_ilp_c.cc main.cc\n",
        "#nvc++   -Wall -O3 -mavx512f -mfma -mp=gpu -cuda -o latency_ilp_c latency_ilp_c.cc main.cc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-035",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* To explore the effect of chasing multiple pointer chains simultaneously, the program has two parameters\n",
        "  * `--n-cycles` : the number of disjoint chain of pointers in the array\n",
        "  * `--n-conc-cycles` : the number of chains of pointers we traverse simultaneously (2 for the code shown just above)\n",
        "* Obviously we need to set the former as large as the latter\n",
        "* Below we simply set them to the same number ($C$)\n",
        "\n",
        "* First let's set $C$ to 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-009",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "export C=1\n",
        "./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-036",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Observe that this case shows a similar performance with the previous version\n",
        "* Now let's set $C$ to 2 and see what happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-010",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "export C=2\n",
        "./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-037",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Observe that `total_accesses` is the same and the execution time is almost halved (i.e., the bandwidth (`bw`) almost doubled)\n",
        "* Play with larger values of `C`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-038",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Make sure that this is not an unintended side effect of changing the way cycles are formed, by setting `--n-cycles 2` and comparing the two cases `--n-conc-cycles 1` and `--n-conc-cycles 2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-011",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "export C=1\n",
        "./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles 2 --n-conc-cycles ${C}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-039",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-2. Plotting C vs. Bandwidth (CPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-012",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=DISABLED\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "m=$((1 << 24))\n",
        "n=$((1 << 27))\n",
        "\n",
        "for C in 1 2 3 4 6 8 10 12 14 16 ; do\n",
        "    echo \"==== C=${C} ====\"\n",
        "    echo ./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}\n",
        "done | tee cpu_bw.txt\n",
        "echo \"done\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-040",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_bw([\"cpu_bw.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-041",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-3. Plotting C vs. Bandwidth (GPU)",
        "\n",
        "* Let's see whether a similar thing happens on the GPU\n",
        "* Set the value of `C` to 1, 2, 3, ... and see the effect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-013",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "export OMP_NUM_THREADS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "export C=1\n",
        "./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-042",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-4. Traversing Multiple Pointer Chains by Multiple Threads on GPU (OpenMP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-043",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* For GPU, a much more straightforward way to increase the bandwidth is, of course, increasing the number of threads\n",
        "* We simply set the number of cycles (`--n-cycles`) and the number of threads to the same number\n",
        "* It lets each thread follow a single pointer chain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-014",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "export m=$((1 << 24))\n",
        "export n=$((1 << 27))\n",
        "export C=1\n",
        "export OMP_NUM_THREADS=${C}\n",
        "./latency --n-elements ${m} --min-accesses ${n} --n-cycles ${C}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-044",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Increase C to 32, 64, ... (a multiple of 32) and see how far you can go\n",
        "\n",
        "* Let's visualize how the bandwidth increases with the number of threads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-015",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "m=$((1 << 24))\n",
        "n=$((1 << 27))\n",
        "\n",
        "for C in 32 64 96 128 192 256 384 512 768 1024 ; do\n",
        "    echo \"==== C=${C} ====\"\n",
        "    OMP_NUM_THREADS=${C} ./latency --n-elements ${m} --min-accesses ${n} --n-cycles ${C}\n",
        "done | tee gpu_bw_threads.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-045",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_bw([\"gpu_bw_threads.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-046",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* <font color=\"red\">NOTE:</font> I found a number of issues and mysteries with `clang++` and `nvc++` in this experiment\n",
        "* `clang++`\n",
        "  * performance is very poor (the bandwidth does not increase almost at all when increasing the number of threads)\n",
        "  * I haven't had the time to look into it\n",
        "* `nvc++`\n",
        "  * bandwidth scales much better than clang++\n",
        "  * but when I give `-O2` option or above in the command line, the compilation succeeds but when executing the commnad, it fails with the following error\n",
        "\n",
        "```\n",
        "$ ./latency --n-elements 16777216 --min-accesses 134217728 --n-cycles 32\n",
        "n_elements : 16777216\n",
        "sz : 134217728 bytes\n",
        "n_cycles : 32\n",
        "len_cycle : 524288\n",
        "n_accesses_per_cycle : 4194304\n",
        "total_accesses : 134217728\n",
        "n_conc_cycles : 1\n",
        "coalese_size : 1\n",
        "seed : 123456789012345\n",
        "Module function not found, error 500\n",
        "Accelerator Fatal Error: Failed to find device function 'nvkernel__Z6cyclesPlllS_llllPi_F1L83_2'! File was compiled with: -gpu=cc80\n",
        "Rebuild this file with -gpu=cc80 to use NVIDIA Tesla GPU 0\n",
        " File: /home/pd/parallel-distributed-programming/jupyter/nb/source/pd07_mem_ext/include/ver/main_omp.cc\n",
        " Function: _Z11make_cyclesPlS_lllll:101\n",
        " Line: 101\n",
        "```\n",
        "\n",
        "* The best workaround for now is to **use nvc++ with `-O1` or below**\n",
        "* Or use CUDA, which I demonstarte below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-047",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-5. Traversing Multiple Pointer Chains by Multiple Threads on GPU (CUDA)",
        "\n",
        "* This would be unnecessary if OpenMP result were more clear-cut\n",
        "\n",
        "* The main file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-048",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile main_cuda.cc\n",
        "#include <assert.h>\n",
        "#include <err.h>\n",
        "#include <getopt.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "#include <time.h>\n",
        "#include \"cuda_util.h\"\n",
        "\n",
        "/* get current time in nanosecond */\n",
        "double cur_time() {\n",
        "  struct timespec ts[1];\n",
        "  int ok = clock_gettime(CLOCK_REALTIME, ts);\n",
        "  if (ok == -1) err(1, \"clock_gettime\");\n",
        "  return ts->tv_sec + ts->tv_nsec * 1e-9;\n",
        "}\n",
        "\n",
        "/* random number generator */\n",
        "struct prng {\n",
        "  long sd;\n",
        "  void seed(long sd) {\n",
        "    this->sd = sd;\n",
        "  }\n",
        "  long gen_randint() {\n",
        "    long a = 0x5DEECE66Dull;\n",
        "    long c = 0xB;\n",
        "    sd = (a * sd + c) & ((1L << 48) - 1);\n",
        "    long y = sd >> 17;\n",
        "    return y;\n",
        "  }\n",
        "};\n",
        "\n",
        "/* allocate on the device the main thing will be run */\n",
        "template<typename T>\n",
        "T * alloc_dev(size_t n_elems) {\n",
        "  size_t sz = sizeof(T) * n_elems;\n",
        "  T * b;\n",
        "  check_cuda_api(cudaMallocManaged((void **)&b, sz));\n",
        "  return b;\n",
        "}\n",
        "\n",
        "template<typename T>\n",
        "void dealloc_dev(T * b) {\n",
        "  check_cuda_api(cudaFree((void *)b));\n",
        "}\n",
        "\n",
        "/* swap a[i] and a[j] */\n",
        "void swap(long * a, long i, long j) {\n",
        "  long ai = a[i];\n",
        "  long aj = a[j];\n",
        "  a[i] = aj;\n",
        "  a[j] = ai;\n",
        "}\n",
        "\n",
        "/* shuffle seq = [0,1,2,...,n_cycles*len_cycle-1];\n",
        "   make sure\n",
        "   (1) coalese_size consecutive elements are sequential.\n",
        "   (2) seq[0:n_cycles] = [0,1,...,n_cycles] */\n",
        "void shuffle(long * seq, long coalese_size,\n",
        "             long n_cycles, long len_cycle, long seed) {\n",
        "  assert(n_cycles % coalese_size == 0);\n",
        "  long m = n_cycles * len_cycle;\n",
        "  for (long i = 0; i < m; i++) {\n",
        "    seq[i] = i;\n",
        "  }\n",
        "  if (seed >= 0) {\n",
        "    prng rg;\n",
        "    rg.seed(seed);\n",
        "    long n_blocks = m / coalese_size;\n",
        "    for (long i = n_cycles / coalese_size; i < n_blocks; i++) {\n",
        "      long j = rg.gen_randint() % (n_blocks - i);\n",
        "      for (long k = 0; k < coalese_size; k++) {\n",
        "        swap(seq, i * coalese_size + k, (i + j) * coalese_size + k);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "/* set a[k] to the next element to access */\n",
        "__host__ __device__\n",
        "void make_cycle(long * a, long * seq,\n",
        "                long start_idx, long n_cycles, long len_cycle) {\n",
        "  // a cycle starting from seq[idx] :\n",
        "  // a[seq[idx]] -> a[seq[idx+n_cycles]] -> a[seq[idx+2*n_cycles]] -> ..\n",
        "  long m = n_cycles * len_cycle;\n",
        "  for (long i = 0; i < len_cycle; i++) {\n",
        "    long cur  = seq[ start_idx +      i  * n_cycles];\n",
        "    long next = seq[(start_idx + (i + 1) * n_cycles) % m];\n",
        "    a[cur] = next;\n",
        "  }\n",
        "}\n",
        "\n",
        "__global__\n",
        "void make_cycles_g(long * a, long * seq, long n_cycles, long len_cycle) {\n",
        "  long nthreads = get_n_threads();\n",
        "  for (long idx = get_thread_index(); idx < n_cycles; idx += nthreads) {\n",
        "    make_cycle(a, seq, idx, n_cycles, len_cycle);\n",
        "  }\n",
        "}\n",
        "\n",
        "void make_cycles(long * a, long * seq, long m,\n",
        "                 long n_cycles, long len_cycle, \n",
        "                 long n_teams, long n_threads_per_team) {\n",
        "  check_cuda_launch((make_cycles_g<<<n_teams,n_threads_per_team>>>(a, seq, n_cycles, len_cycle)));\n",
        "}\n",
        "\n",
        "/* defined in a separate file (latency.cc or latency_ilp.cc) */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx);\n",
        "\n",
        "struct opts {\n",
        "  /* number of elements */\n",
        "  long n_elements;\n",
        "  /* minimum number of scans */\n",
        "  double min_scans;\n",
        "  /* minimum number of accesses */\n",
        "  long min_accesses;\n",
        "  /* number of consecutive elements guaranteed to be contiguous */\n",
        "  long coalese_size;\n",
        "  long n_cycles;\n",
        "  long n_conc_cycles;\n",
        "  long seed;\n",
        "  opts() {\n",
        "    n_elements = 1L << 24;\n",
        "    min_scans = 5.3;\n",
        "    min_accesses = (1 << 20);\n",
        "    coalese_size = 1;\n",
        "    n_cycles = 1;\n",
        "    n_conc_cycles = 1;\n",
        "    seed = 123456789012345L;\n",
        "  }\n",
        "};\n",
        "\n",
        "void usage(char * prog) {\n",
        "  opts o;\n",
        "  fprintf(stderr, \"usage:\\n\");\n",
        "  fprintf(stderr, \"  %s [options]\\n\", prog);\n",
        "  fprintf(stderr, \"options:\\n\");\n",
        "  fprintf(stderr, \"  -m,--n-elements (%ld)\\n\", o.n_elements);\n",
        "  fprintf(stderr, \"  --min-scans N (%.3f)\\n\", o.min_scans);\n",
        "  fprintf(stderr, \"  --min-accesses N (%ld)\\n\", o.min_accesses);\n",
        "  fprintf(stderr, \"  -c,--coalese-size N (%ld)\\n\", o.coalese_size);\n",
        "  fprintf(stderr, \"  --n-cycles N (%ld)\\n\", o.n_cycles);\n",
        "  fprintf(stderr, \"  --seed N (%ld)\\n\", o.seed);\n",
        "}\n",
        "\n",
        "opts parse_opts(int argc, char ** argv) {\n",
        "  static struct option long_options[] = {\n",
        "    {\"n-elements\",          required_argument, 0, 'm' },\n",
        "    {\"min-scans\",           required_argument, 0, 0 },\n",
        "    {\"min-accesses\",        required_argument, 0, 0 },\n",
        "    {\"coalese-size\",        required_argument, 0, 0 },\n",
        "    {\"n-cycles\",            required_argument, 0, 0 },\n",
        "    {\"n-conc-cycles\",       required_argument, 0, 0 },\n",
        "    {\"seed\",                required_argument, 0, 0 },\n",
        "    {0,         0,                 0,  0 }\n",
        "  };\n",
        "  opts o;\n",
        "  while (1) {\n",
        "    int option_index = 0;\n",
        "    int c = getopt_long(argc, argv, \"m:c:\",\n",
        "\t\t\tlong_options, &option_index);\n",
        "    if (c == -1) break;\n",
        "\n",
        "    switch (c) {\n",
        "    case 'm':\n",
        "      o.n_elements = atol(optarg);\n",
        "      break;\n",
        "    case 0:\n",
        "      {\n",
        "        const char * opt_name = long_options[option_index].name;\n",
        "        if (strcmp(opt_name, \"seed\") == 0) {\n",
        "          o.seed = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"min-scans\") == 0) {\n",
        "          o.min_scans = atof(optarg);\n",
        "        } else if (strcmp(opt_name, \"min-accesses\") == 0) {\n",
        "          o.min_accesses = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"coalese-size\") == 0) {\n",
        "          o.coalese_size = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"n-cycles\") == 0) {\n",
        "          o.n_cycles = atol(optarg);\n",
        "        } else if (strcmp(opt_name, \"n-conc-cycles\") == 0) {\n",
        "          o.n_conc_cycles = atol(optarg);\n",
        "        } else {\n",
        "          usage(argv[0]);\n",
        "          exit(1);\n",
        "        }\n",
        "        break;\n",
        "      }\n",
        "    default:\n",
        "      usage(argv[0]);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "  return o;\n",
        "}\n",
        "\n",
        "long getenv_long(const char * s) {\n",
        "  char * vs = getenv(s);\n",
        "  if (!vs) {\n",
        "    fprintf(stderr, \"set environment variable %s\\n\", s);\n",
        "    exit(0);\n",
        "  }\n",
        "  return atol(vs);\n",
        "}\n",
        "\n",
        "typedef long longv;\n",
        "\n",
        "int main(int argc, char ** argv) {\n",
        "  const long L = sizeof(longv) / sizeof(long);\n",
        "  long n_teams = getenv_long(\"OMP_NUM_TEAMS\");\n",
        "  long n_threads_per_team = getenv_long(\"OMP_NUM_THREADS\");\n",
        "  opts opt = parse_opts(argc, argv);\n",
        "  long m = opt.n_elements;\n",
        "  long n_cycles = opt.n_cycles;\n",
        "  long coalese_size = opt.coalese_size;\n",
        "  assert(n_cycles % coalese_size == 0);\n",
        "  assert(coalese_size % L == 0);\n",
        "  long len_cycle = (m + n_cycles - 1) / n_cycles;\n",
        "  if (m % n_cycles) {\n",
        "    fprintf(stderr,\n",
        "            \"WARNING : m (%ld) not divisible by n_cycles (%ld),\"\n",
        "            \" rounded up to %ld\\n\",\n",
        "            m, n_cycles, len_cycle * n_cycles);\n",
        "    m = len_cycle * n_cycles;\n",
        "  }\n",
        "  printf(\"n_elements : %ld\\n\", m);\n",
        "  size_t sz = sizeof(long) * m;\n",
        "  printf(\"sz : %ld bytes\\n\", sz);\n",
        "  printf(\"n_cycles : %ld\\n\", n_cycles);\n",
        "  printf(\"len_cycle : %ld\\n\", len_cycle);\n",
        "  double s = opt.min_scans;\n",
        "  long n = len_cycle * s;\n",
        "  if (n * n_cycles < opt.min_accesses) {\n",
        "    n = (opt.min_accesses + n_cycles - 1) / n_cycles;\n",
        "  }\n",
        "  printf(\"n_accesses_per_cycle : %ld\\n\", n);\n",
        "  printf(\"total_accesses : %ld\\n\", n * n_cycles);\n",
        "  long n_conc_cycles = opt.n_conc_cycles;\n",
        "  printf(\"n_conc_cycles : %ld\\n\", n_conc_cycles);\n",
        "  assert(n_cycles % n_conc_cycles == 0);\n",
        "  printf(\"coalese_size : %ld\\n\", coalese_size);\n",
        "  printf(\"seed : %ld\\n\", opt.seed);\n",
        "\n",
        "  long * seq = alloc_dev<long>(m);\n",
        "  shuffle(seq, coalese_size, n_cycles, len_cycle, opt.seed);\n",
        "\n",
        "  long * a = alloc_dev<long>(m);\n",
        "  double t0 = cur_time();\n",
        "  make_cycles(a, seq, m, n_cycles, len_cycle, \n",
        "              n_teams, n_threads_per_team);\n",
        "  double t1 = cur_time();\n",
        "  double dt0 = t1 - t0;\n",
        "  printf(\"make_cycles_total : %f sec\\n\", dt0);\n",
        "  printf(\"make_cycles_per_elem : %.1f nsec\\n\", 1.0e9 * dt0 / m);\n",
        "  long * end = alloc_dev<long>(n_cycles);\n",
        "  int * thread_idx = alloc_dev<int>(n_cycles);\n",
        "  double t2 = cur_time();\n",
        "  cycles(a, m, n, end, n_cycles, n_conc_cycles,\n",
        "         n_teams, n_threads_per_team, thread_idx);\n",
        "  double t3 = cur_time();\n",
        "  double dt1 = t3 - t2;\n",
        "  long bytes = sizeof(long) * n * n_cycles;\n",
        "  double bw = bytes / dt1;\n",
        "  printf(\"bytes accessed : %ld bytes\\n\", bytes);\n",
        "  printf(\"time_total : %f sec\\n\", dt1);\n",
        "  printf(\"time_per_access : %.1f nsec/access\\n\", 1.0e9 * dt1 / (n * n_cycles));\n",
        "  printf(\"bw : %.3f GB/sec\\n\", bw * 1.e-9);\n",
        "  printf(\"checking results ... \"); fflush(stdout);\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    assert(end[idx] == seq[(idx + n * n_cycles) % m]);\n",
        "  }\n",
        "  printf(\"OK\\n\");\n",
        "#if 0\n",
        "  for (long idx = 0; idx < n_cycles; idx++) {\n",
        "    printf(\"idx = %ld by thread %d\\n\", idx, thread_idx[idx]);\n",
        "  }\n",
        "#endif\n",
        "  dealloc_dev(seq);\n",
        "  dealloc_dev(end);\n",
        "  dealloc_dev(a);\n",
        "  return 0;\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-049",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* The core part of the program that accesses the array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-050",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "%%writefile latency_cuda.cc\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#include \"cuda_util.h\"\n",
        "\n",
        "/* starting from cell &a[idx], chase ->next ptr n times\n",
        "   and put where it ends in end[idx] */\n",
        "__host__ __device__\n",
        "void cycle(long * a, long idx, long n, long * end, int * thread_idx) {\n",
        "  long k = idx;\n",
        "  asm volatile(\"// ========== loop begins ========== \");\n",
        "#pragma unroll(8)\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    k = a[k];\n",
        "  }\n",
        "  asm volatile(\"// ---------- loop ends ---------- \");\n",
        "  end[idx] = k;\n",
        "  thread_idx[idx] = k;\n",
        "}\n",
        "\n",
        "__global__ void cycles_g(long * a, long n_cycles, long n, long * end, int * thread_idx) {\n",
        "  long nthreads = get_n_threads();\n",
        "  for (long idx = get_thread_index(); idx < n_cycles; idx += nthreads) {\n",
        "    cycle(a, idx, n, end, thread_idx);\n",
        "  }\n",
        "}\n",
        "\n",
        "/* a is an array of m cells;\n",
        "   starting from &a[idx] for each idx in [0:n_cycles],\n",
        "   chase ->next ptr n times and put where it ends in end[idx] */\n",
        "void cycles(long * a, long m, long n, long * end, long n_cycles,\n",
        "            long n_conc_cycles,\n",
        "            long n_teams, long n_threads_per_team, int * thread_idx) {\n",
        "  check_cuda_launch((cycles_g<<<n_teams,n_threads_per_team>>>(a, n_cycles, n, end, thread_idx)));\n",
        "}\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-051",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Compile them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-016",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "nvcc -DDBG=0 -O4 -x cu -o latency_cuda latency_cuda.cc main_cuda.cc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-017",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "export OMP_TARGET_OFFLOAD=MANDATORY\n",
        "export OMP_NUM_TEAMS=1\n",
        "m=$((1 << 24))\n",
        "n=$((1 << 27))\n",
        "\n",
        "for C in 32 64 96 128 192 256 384 512 768 1024 ; do\n",
        "    echo \"==== C=${C} ====\"\n",
        "    OMP_NUM_THREADS=${C} ./latency_cuda --n-elements ${m} --min-accesses ${n} --n-cycles ${C}\n",
        "done | tee gpu_bw_threads.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-052",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-053",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "import vis_mem\n",
        "vis_mem.vis_bw([\"gpu_bw_threads.txt\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "SoS",
      "language": "sos",
      "name": "sos"
    },
    "language_info": {
      "codemirror_mode": "sos",
      "file_extension": ".sos",
      "mimetype": "text/x-sos",
      "name": "sos",
      "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
      "pygments_lexer": "sos"
    },
    "sos": {
      "kernels": [
        [
          "Bash",
          "bash",
          "bash",
          "",
          "shell"
        ],
        [
          "C",
          "c_kernel",
          "c",
          "",
          ""
        ],
        [
          "Go",
          "gophernotes",
          "go",
          "",
          ""
        ],
        [
          "Julia 1.10.2",
          "julia-1.10",
          "julia",
          "",
          ""
        ],
        [
          "OCaml default",
          "ocaml-jupyter",
          "OCaml",
          "",
          "text/x-ocaml"
        ],
        [
          "Python 3 (ipykernel)",
          "python3",
          "python3",
          "",
          {
            "name": "ipython",
            "version": 3
          }
        ],
        [
          "Rust",
          "rust",
          "rust",
          "",
          ""
        ]
      ],
      "panel": {
        "displayed": true,
        "height": 0
      },
      "version": "0.23.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}