{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-001",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "\n",
        "# 1. GPU Scheduling ",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-002",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "\n",
        "Enter your name and student ID.\n",
        "\n",
        " * Name:\n",
        " * Student ID:\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-003",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "\n",
        "This notebook demonstrates how NVIDIA GPU is scheduling threads.\n",
        "\n",
        "# 2. Compilers",
        "\n",
        "* This exercise is CUDA-specific, so we use NVIDIA CUDA\n",
        "\n",
        "* Execute this before you use CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-004",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "export PATH=/usr/local/cuda/bin:$PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-005",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Check if it works (check if full paths of nvcc are shown)\n",
        "* We do not recommend nvc/nvc++ for this exercise, but you might give them a try if you like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-006",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "which nvcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-007",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 3. Check host and GPU",
        "\n",
        "* Check if you are using the right host, tauleg000, <font color=\"red\">not taulec</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-008",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "hostname\n",
        "hostname | grep tauleg || echo \"Oh, you are not on the right host, access https://tauleg000.zapto.org/ instead\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-009",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Check if GPU is alive by nvidia-smi\n",
        "* Do `nvidia-smi --help` or see manual (`man nvidia-smi` on terminal) for more info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-010",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-011",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "\n",
        "# 4. Basics",
        "\n",
        "When you call a kernel (function f) with\n",
        "```\n",
        "f<<<nb,bs>>>();\n",
        "```\n",
        "it creates (_nb * bs_) CUDA threads.\n",
        "\n",
        "More precisely, it creates _nb_ thread blocks, each of which has _bs_ CUDA threads.\n",
        "\n",
        "The following code is a tool to record how threads are executed on GPU.\n",
        "\n",
        "It creates many threads repeating a trivial (useless) computation x = a * x + b many times.\n",
        "Each thread occasionally records the clock to record when and where these threads progress over time.\n",
        "\n",
        "Specifically,\n",
        "\n",
        "```\n",
        "./cuda_sched_rec NTHREADS THREAD_BLOCK_SZ N M \n",
        "```\n",
        "\n",
        "creates approximately NTHREADS threads, with THREAD_BLOCK_SZ threads in each thread block (the number of threads is not exactly NTHREADS when it is not a multiple of THREAD_BLOCK_SZ).\n",
        "\n",
        "* Each thread repeats x = A x + B, (N * M) times.\n",
        "* Each thread records clock N times (every M iterations).\n",
        "\n",
        "* At the end of execution, it dumps the results in the following format for each line.\n",
        "\n",
        "```\n",
        "thread=<idx> x=<ans> sm0=<starting SM> sm1=<ending SM> t0 t1 t2 ... t_{n-1}\n",
        "```\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-001",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "%%writefile cuda_sched_rec.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// error check utility (check_api_error and check_launch_error)\n",
        "#include \"cuda_util.h\"\n",
        "\n",
        "// record of execution\n",
        "typedef long long int llint;\n",
        "typedef struct {\n",
        "  double x;                     // a (meaningless) answer \n",
        "  uint sm0;                     // SM on which a thread got started\n",
        "  uint sm1;                     // SM on which a thread ended (MUST BE = sm0; just to verify that)\n",
        "} record_t;\n",
        "\n",
        "/* this thread repeats x = a x + b (N * M) times.\n",
        "   it records the clock N times (every M iterations of x = a x + b)\n",
        "   to array T.\n",
        "   final result of x = a x + b, as well as SM each thread was executed\n",
        "   on are recorded to R. */\n",
        "__global__ void cuda_thread_fun(double a, double b, record_t * R,\n",
        "                                llint * T, llint n, llint m,\n",
        "                                int nthreads) {\n",
        "  // my thread index\n",
        "  int idx      = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if (idx >= nthreads) return;\n",
        "  // initial value (not important)\n",
        "  double x = idx;\n",
        "  // where clocks are recorded\n",
        "  T = &T[idx * n];\n",
        "  // record starting SM\n",
        "  R[idx].sm0 = get_smid();\n",
        "  // main thing. repeat a x + b many times,\n",
        "  // occasionally recording the clock\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    T[i] = clock64();\n",
        "    for (long j = 0; j < m; j++) {\n",
        "      x = a * x + b;\n",
        "    }\n",
        "  }\n",
        "  // record ending SM (must be = sm0)\n",
        "  R[idx].sm1 = get_smid();\n",
        "  // record result, just so that the computation is not\n",
        "  // eliminated by the compiler\n",
        "  R[idx].x = x;\n",
        "}\n",
        "\n",
        "/* usage\n",
        "   ./cuda_sched NTHREADS THREAD_BLOCK_SZ N M S A B\n",
        "\n",
        "   creates about NTHREADS threads, with THREAD_BLOCK_SZ\n",
        "   threads in each thread block. \n",
        "   each thread repeats x = A x + B (N * M) times.\n",
        "\n",
        "   S is the shared memory allocated for each thread block\n",
        "   (just to control the number of thread blocks simultaneously\n",
        "   scheduled on an SM). shared memory is not actually used at all.\n",
        " */\n",
        "int main(int argc, char ** argv) {\n",
        "  int i = 1;\n",
        "  int nthreads        = (argc > i ? atoi(argv[i])  : 100);  i++;\n",
        "  int thread_block_sz = (argc > i ? atoi(argv[i])  : 64);   i++;\n",
        "  llint n             = (argc > i ? atoll(argv[i]) : 100);  i++;\n",
        "  llint m             = (argc > i ? atoll(argv[i]) : 100);  i++;\n",
        "  int D               = (argc > i ? atoll(argv[i]) : 1);    i++;\n",
        "  int shm_sz          = (argc > i ? atoi(argv[i])  : 0);    i++;\n",
        "  double a            = (argc > i ? atof(argv[i])  : 0.99); i++;\n",
        "  double b            = (argc > i ? atof(argv[i])  : 1.00); i++;\n",
        "\n",
        "  // get the required number of thread blocks\n",
        "  int n_thread_blocks = (nthreads + thread_block_sz - 1) / thread_block_sz;\n",
        "  printf(\"%d threads/block * %d blocks\\n\", thread_block_sz, n_thread_blocks);\n",
        "\n",
        "  // allocate record_t array (both on host and device)\n",
        "  long R_sz = sizeof(record_t) * nthreads;\n",
        "  record_t * R = (record_t *)calloc(R_sz, 1);\n",
        "  record_t * R_dev;\n",
        "  check_api_error(cudaMalloc(&R_dev, R_sz));\n",
        "  check_api_error(cudaMemcpy(R_dev, R, R_sz, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // allocate clock array (both on host and device)\n",
        "  long T_sz = sizeof(llint) * n * nthreads;\n",
        "  llint * T = (llint *)calloc(T_sz, 1);\n",
        "  llint * T_dev;\n",
        "  check_api_error(cudaMalloc(&T_dev, T_sz));\n",
        "  check_api_error(cudaMemcpy(T_dev, T, T_sz, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // call the kernel\n",
        "  int shm_elems = shm_sz / sizeof(double);\n",
        "  int shm_size = shm_elems * sizeof(double);\n",
        "  check_launch_error((cuda_thread_fun<<<n_thread_blocks,thread_block_sz,shm_size>>>\n",
        "                      (a, b, R_dev, T_dev, n, m, nthreads)));\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // get back the results and clocks\n",
        "  check_api_error(cudaMemcpy(R, R_dev, R_sz, cudaMemcpyDeviceToHost));\n",
        "  check_api_error(cudaMemcpy(T, T_dev, T_sz, cudaMemcpyDeviceToHost));\n",
        "  // dump the for visualization\n",
        "  long k = 0;\n",
        "  for (long idx = 0; idx < nthreads; idx++) {\n",
        "    printf(\"thread=%ld x=%f sm0=%u sm1=%u\", idx, R[idx].x, R[idx].sm0, R[idx].sm1);\n",
        "    for (long i = 0; i < n; i++) {\n",
        "      printf(\" %lld\", T[k]);\n",
        "      k++;\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-012",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Read the code carefully and understand what it is doing.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-002",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "nvcc --generate-code arch=compute_80,code=sm_80 -o cuda_sched_rec cuda_sched_rec.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-003",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "./cuda_sched_rec 64 32 10 100 | head -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-013",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 5. Visualization",
        "\n",
        "* The following python code parses and visualizes the output of cuda_sched_rec.\n",
        "* The code is shown below for your information; you don't have to understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-004",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "#!/usr/bin/python3\n",
        "import re\n",
        "# from matplotlib import collections  as mc\n",
        "import matplotlib.collections as mc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def read_dat(files_dat):\n",
        "    pat = re.compile(\"thread=(?P<thread>\\d+) x=(?P<x>\\d+\\.\\d+) sm0=(?P<sm0>\\d+) sm1=(?P<sm1>\\d+)(?P<t>( \\d+)*)\")\n",
        "    log = {}\n",
        "    for file_dat in files_dat:\n",
        "        with open(file_dat) as fp:\n",
        "            for line in fp:\n",
        "                # 1 : 100.000000 20524414966449 20524423007875 0 0\n",
        "                m = pat.match(line)\n",
        "                if not m:\n",
        "                    continue\n",
        "                thread = int(m.group(\"thread\"))\n",
        "                x      = float(m.group(\"x\"))\n",
        "                sm0    = int(m.group(\"sm0\"))\n",
        "                sm1    = int(m.group(\"sm1\"))\n",
        "                t      = [int(s) for s in m.group(\"t\").strip().split()]\n",
        "                assert(sm0 == sm1), (sm0, sm1)\n",
        "                if sm0 not in log:\n",
        "                    log[sm0] = []\n",
        "                log[sm0].append((thread, t))\n",
        "    return log\n",
        "\n",
        "def cuda_sched_plt(files_dat, start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\")):\n",
        "    log = read_dat(files_dat)\n",
        "    n_sms = max(sm for sm in log) + 1\n",
        "    cmap = plt.cm.get_cmap('RdYlGn', n_sms)\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.xlabel(\"cycles\")\n",
        "    plt.ylabel(\"thread idx\")\n",
        "    for sm,records in sorted(list(log.items())):\n",
        "        T0 = min(T[0] for thread, T in records)\n",
        "        X = []\n",
        "        Y = []\n",
        "        sm_color = cmap(sm)\n",
        "        for thread, T in records:\n",
        "            if start_thread <= thread < end_thread:\n",
        "                for t in T:\n",
        "                    if start_t <= t - T0 <= end_t:\n",
        "                        X.append(t - T0)\n",
        "                        Y.append(thread)\n",
        "        ax.plot(X, Y, 'o', markersize=0.5, color=sm_color)\n",
        "    ax.autoscale()\n",
        "    plt.savefig(\"sched.svg\")\n",
        "    plt.show()\n",
        "    \n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-014",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Let's visualize a few configurations.\n",
        "\n",
        "## 5-1. one thread"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-005",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "./cuda_sched_rec 1 1 100 1000 > cs_1_1.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-006",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_1_1.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-015",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* you can change `start_t` and `end_t` to zoom into a narrower time interval and change `start_thread` and `end_thread` to zoom into a range of threads\n",
        "* or, you can open `sched.svg` generated along with the visualization and magnify anywhere you want to look into, either by the browser or any SVG viewer on your PC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-016",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-2. many threads with 1 thread/block",
        "\n",
        "* play with changing N to other values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-007",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "N=150\n",
        "./cuda_sched_rec ${N} 1 100 1000 > cs_N_1.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-008",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_N_1.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-017",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Increase N and observe when the execution time (the time at the right end of the graph) starts to increase.\n",
        "* Even in that case, all N threads appear to be executing simultaneously (not one after another).\n",
        "* That is, _hardware_ interleaves execution of these threads, rapidly switching from one to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-018",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-3. many threads in 1 thread block",
        "\n",
        "* play with changing N to other values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-009",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "N=150\n",
        "./cuda_sched_rec ${N} ${N} 100 1000 > cs_N_N.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-010",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_N_N.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-019",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Observe that they are always executed on the same SM. You are not utilizing multiple SMs at all.\n",
        "\n",
        "* There is a hardwired limit on the number of threads per block. Try to find it and then confirm it with Google.\n",
        "\n",
        "* When increasing N, observe when the execution time starts to increase. Why do you think it doesn't immediately increase with N&gt;1?\n",
        "\n",
        "* With a modest value of N (say 100), zoom in at either end of the execution and observe whether there is _any_ difference on when they started or finished execution.  If you look carefully, you will notice that a number of consecutive threads start and end _exactly the same clock_.  Those threads are called a _warp_ and they share an instruction pointer.  It is very analogous to SIMD instruction found in CPUs that apply the same operation on multiple operands.  Guess the number of threads of a warp from the experimental results and confirm it by Google."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-020",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 5-4. many threads in many threads/block",
        "\n",
        "* play with changing N and B to other values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-011",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "N=150\n",
        "B=64\n",
        "./cuda_sched_rec ${N} ${B} 100 1000 > cs_N_B.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-012",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_N_B.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-021",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Try to find the maximum number of threads that does not increase the execution time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-022",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 6. Thread blocks",
        "\n",
        "A thread block is the unit of dispatching to a streaming multiprocessor (SM), which is like a physical core of a CPU.  Threads within a thread block are always dispatched together to the same SM and once dispatched stay on the same SM until finished.\n",
        "\n",
        "* see [CUDA C++ Programming Guide: A Scalable Programming Model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#scalable-programming-model)\n",
        "\n",
        "An SM is a highly multithreaded processor, which can accommodate many threads at the same time and interleave them  by hardware.  For example, it can easily hold, say, 500 threads and interleave their execution without involving software.  In terms of hardware capability, it is somewhat similar to simultaneous multithreading (SIMT) of CPUs.  The degree of multithreading is very different, however; Intel CPUs normally support only two hardware threads (virtual cores) on each physical core.  Moreover, software (either operating system or user-level software) needs to designate which virtual core you want to run a thread on.  In a sense, CPU exposes each virtual core as a single-threaded machine.  If you put more than one OpenMP (OS-level) thread on the same virtual core, software should switch between them from time to time.  A streaming multiprocessor of a GPU, in contrast, is a machine that literally takes many threads and concurrently executes them by hardware.  Determining the SM a thread block executes on is done by hardware.\n",
        "\n",
        "How many thread blocks are scheduled on an SM at the same time?  It depends; it depends on how much \"resources\" a single thread block requires.  Here, \"resources\" mean two things.\n",
        "\n",
        "1. registers\n",
        "1. shared memory (see below)\n",
        "\n",
        "\n",
        "_Registers_ are used for holding local variables and intermediate results of computation.  How many registers a thread block uses is not something you can reliably determine by looking at your code; it depends on the code generated by the compiler.  You can know it by passing `-Xptxas -v` to nvcc and looking at the compiler message.\n",
        "\n",
        "_Shared memory_ is a scratch-pad memory only shared within a single thread block.  Physically, you can consider it to be a small fast memory attached to each SM.  The name \"shared memory\" is clearly a misnomer; ordinary memory you get by `cudaMalloc` _is_ shared by all threads (called \"global memory\").  In contrast, shared memory is, contrary to its name, shared only among threads within a single thread block.  \"Local memory\" (as opposed to global memory) would have been a better name for it, IMO.\n",
        "\n",
        "Both registers and shared memory for a thread block are kept on physical registers/memory of an SM throughout the lifetime of the thread block.  Thus, accommodating a larger number of thread blocks at the same time requires a proportionally larger amount of registers/shared memory, which is subject to the physical resource limit of an SM.\n",
        "\n",
        "Each SM has the following physical resources.\n",
        "\n",
        "|       | registers      |  shared memory  |\n",
        "|-------|----------------|-----------------|\n",
        "|Pascal | 32 bit x 65536 |  64KB           |\n",
        "|Volta  | 32 bit x 65536 |  up to 96KB (*) |\n",
        "|Ampere | 32 bit x 65536 |  up to 163KB    |\n",
        "\n",
        "(*) configurable subject to L1 cache + shared memory <= 128KB and shared memory <= 96KB\n",
        "\n",
        "* [Pascal Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html#sm-occupancy)\n",
        "* [Volta Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/volta-tuning-guide/index.html#sm-occupancy)\n",
        "* [NVIDIA Ampere GPU Architecture Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html#sm-occupancy)\n",
        "\n",
        "By default, a thread does not use shared memory at all.\n",
        "\n",
        "Let's observe how many registers a thread uses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-013",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "nvcc --generate-code arch=compute_80,code=sm_80 -Xptxas -v -o cuda_sched_rec cuda_sched_rec.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-023",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Since the computation is very simple, register usage will not be a limiting factor for this computation.\n",
        "Also, since it does not use shared memory at all, it won't be a limiting factor either.\n",
        "Only the hardwired limit is the limiting factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-024",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 7. Shared memory",
        "\n",
        "* Let's use shared memory to observe how it affects the number of thread blocks simultaneously executed.\n",
        "You specify the size of shared memory per thread block via the third parameter of kernel call, like this.\n",
        "\n",
        "```\n",
        "f<<<nb,bs,S>>>();\n",
        "```\n",
        "\n",
        "The above kernel launch statement specifies that $S$ bytes of shared memory should be allocated to _each thread block_.  Each SM can therefore execute only up to (SHARED_MEMORY_SIZE_PER_SM / $S$) thread blocks simultaneously.\n",
        "\n",
        "You can get a pointer to the part of the shared memory allocated to each thread via the following strange syntax within your kernel function, though it is not necessary in our current experiment.\n",
        "\n",
        "```\n",
        "extern __shared__ T shmem[];\n",
        "```\n",
        "\n",
        "With that, `shmem` points to the start of the shared memory for the thread block.  The name can be arbitrary.\n",
        "\n",
        "`cuda_sched_rec.cu` is already written to take the size of the shared memory per thread block as a parameter.\n",
        "\n",
        "Let's allocate 32KB for each thread block; then, on Ampere, only up to three thread blocks (163KB/32KB) can be executed simultaneously.\n",
        "\n",
        "The following creates 100 thread blocks (in order to avoid creating too many threads, it will set the thread per block to an unusual value of one)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-014",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "N=150\n",
        "S=$((32 * 1024))\n",
        "./cuda_sched_rec ${N} 1 100 1000 ${S} > cs_N_1_S.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-025",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Before visualizing it, imagine what it is like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-015",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_N_1_S.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-026",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "* Play with changing $N$ above; predict when thread blocks start executing not simultaneously (one after another) and confirm it by the experiment (hint: Ampere has 108 streaming multiprocessors).\n",
        "* Change $S$ and see how it affects the above threshold value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-027",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# 8. Warp",
        "\n",
        "* Consecutively numbered 32 threads within a thread block makes a _warp_ and they can execute only one same instruction at a time.\n",
        "* That is, it's not possible, within a single cycle, for some threads to execute an instruction A while others in the same warp execute another instruction B.  All the GPU can do is simply to keep some threads from executing instructions that they should not execute.\n",
        "* A typical example is an \"if\" statement. e.g.,\n",
        "```\n",
        "if (thread_idx % 2 == 0) {\n",
        "  A;\n",
        "} else {\n",
        "  B;\n",
        "}\n",
        "```\n",
        "If there are _any_ thread executing A and _any_ thread executing B within a warp, the time the warp takes is the time to execute A _plus_ the time to execute B.\n",
        "* An important performance implication is you'd better not have threads branching differently within the same warp.\n",
        "\n",
        "* Change the following code as follows.\n",
        "  * it takes an additional command line parameter D\n",
        "  * each thread executes the loop\n",
        "```\n",
        "      for (long j = 0; j < m; j++) {\n",
        "        x = a * x + b;\n",
        "      }\n",
        "```\n",
        "when and only when (idx / D) is an odd number.\n",
        "  * for example, if D is 1, then all even-numbered threads execute the loop and all odd-numbered threads do not execute it\n",
        "  * if D is 32, for example, (idx / D) is essentially the \"warp index\"; even-numbered warps execute the loop and odd-numbered warps skip it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-016",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "%%writefile cuda_sched_rec_warp.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// error check utility (check_api_error and check_launch_error)\n",
        "#include \"cuda_util.h\"\n",
        "\n",
        "// record of execution\n",
        "typedef long long int llint;\n",
        "typedef struct {\n",
        "  double x;                     // a (meaningless) answer \n",
        "  uint sm0;                     // SM on which a thread got started\n",
        "  uint sm1;                     // SM on which a thread ended (MUST BE = sm0; just to verify that)\n",
        "} record_t;\n",
        "\n",
        "/* this thread repeats x = a x + b (N * M) times.\n",
        "   it records the clock N times (every M iterations of x = a x + b)\n",
        "   to array T.\n",
        "   final result of x = a x + b, as well as SM each thread was executed\n",
        "   on are recorded to R. */\n",
        "__global__ void cuda_thread_fun(double a, double b, record_t * R,\n",
        "                                llint * T, llint n, llint m,\n",
        "                                int D,\n",
        "                                int nthreads) {\n",
        "  // my thread index\n",
        "  int idx      = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "  if (idx >= nthreads) return;\n",
        "  // initial value (not important)\n",
        "  double x = idx;\n",
        "  // where clocks are recorded\n",
        "  T = &T[idx * n];\n",
        "  // record starting SM\n",
        "  R[idx].sm0 = get_smid();\n",
        "  // main thing. repeat a x + b many times,\n",
        "  // occasionally recording the clock\n",
        "  for (long i = 0; i < n; i++) {\n",
        "    T[i] = clock64();\n",
        "    if ((idx / D) % 2 == 0) {\n",
        "      for (long j = 0; j < m; j++) {\n",
        "        x = a * x + b;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  // record ending SM (must be = sm0)\n",
        "  R[idx].sm1 = get_smid();\n",
        "  // record result, just so that the computation is not\n",
        "  // eliminated by the compiler\n",
        "  R[idx].x = x;\n",
        "}\n",
        "\n",
        "/* usage\n",
        "   ./cuda_sched NTHREADS THREAD_BLOCK_SZ N M S A B\n",
        "\n",
        "   creates about NTHREADS threads, with THREAD_BLOCK_SZ\n",
        "   threads in each thread block. \n",
        "   each thread repeats x = A x + B (N * M) times.\n",
        "\n",
        "   S is the shared memory allocated for each thread block\n",
        "   (just to control the number of thread blocks simultaneously\n",
        "   scheduled on an SM). shared memory is not actually used at all.\n",
        " */\n",
        "int main(int argc, char ** argv) {\n",
        "  int i = 1;\n",
        "  int nthreads        = (argc > i ? atoi(argv[i])  : 100);  i++;\n",
        "  int thread_block_sz = (argc > i ? atoi(argv[i])  : 64);   i++;\n",
        "  llint n             = (argc > i ? atoll(argv[i]) : 100);  i++;\n",
        "  llint m             = (argc > i ? atoll(argv[i]) : 100);  i++;\n",
        "  int D               = (argc > i ? atoll(argv[i]) : 1);    i++;\n",
        "  int shm_sz          = (argc > i ? atoi(argv[i])  : 0);    i++;\n",
        "  double a            = (argc > i ? atof(argv[i])  : 0.99); i++;\n",
        "  double b            = (argc > i ? atof(argv[i])  : 1.00); i++;\n",
        "\n",
        "  // get the required number of thread blocks\n",
        "  int n_thread_blocks = (nthreads + thread_block_sz - 1) / thread_block_sz;\n",
        "  printf(\"%d threads/block * %d blocks\\n\", thread_block_sz, n_thread_blocks);\n",
        "\n",
        "  // allocate record_t array (both on host and device)\n",
        "  long R_sz = sizeof(record_t) * nthreads;\n",
        "  record_t * R = (record_t *)calloc(R_sz, 1);\n",
        "  record_t * R_dev;\n",
        "  check_api_error(cudaMalloc(&R_dev, R_sz));\n",
        "  check_api_error(cudaMemcpy(R_dev, R, R_sz, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // allocate clock array (both on host and device)\n",
        "  long T_sz = sizeof(llint) * n * nthreads;\n",
        "  llint * T = (llint *)calloc(T_sz, 1);\n",
        "  llint * T_dev;\n",
        "  check_api_error(cudaMalloc(&T_dev, T_sz));\n",
        "  check_api_error(cudaMemcpy(T_dev, T, T_sz, cudaMemcpyHostToDevice));\n",
        "\n",
        "  // call the kernel\n",
        "  int shm_elems = shm_sz / sizeof(double);\n",
        "  int shm_size = shm_elems * sizeof(double);\n",
        "  check_launch_error((cuda_thread_fun<<<n_thread_blocks,thread_block_sz,shm_size>>>\n",
        "                      (a, b, R_dev, T_dev, n, m, D, nthreads)));\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // get back the results and clocks\n",
        "  check_api_error(cudaMemcpy(R, R_dev, R_sz, cudaMemcpyDeviceToHost));\n",
        "  check_api_error(cudaMemcpy(T, T_dev, T_sz, cudaMemcpyDeviceToHost));\n",
        "  // dump the for visualization\n",
        "  long k = 0;\n",
        "  for (long idx = 0; idx < nthreads; idx++) {\n",
        "    printf(\"thread=%ld x=%f sm0=%u sm1=%u\", idx, R[idx].x, R[idx].sm0, R[idx].sm1);\n",
        "    for (long i = 0; i < n; i++) {\n",
        "      printf(\" %lld\", T[k]);\n",
        "      k++;\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-017",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "nvcc --generate-code arch=compute_80,code=sm_80 -o cuda_sched_rec_warp cuda_sched_rec_warp.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "kernel": "SoS",
        "nbgrader": {
          "grade": false,
          "grade_id": "c-028",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Execute the code with various D's (and perhaps other parameters) to visualize the effect of warps and its performance implication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Bash",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-018",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "N=256\n",
        "./cuda_sched_rec_warp ${N} 32 100 1000 1 > cs_warp.dat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "kernel": "Python 3 (ipykernel)",
        "nbgrader": {
          "grade": true,
          "grade_id": "p-019",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false,
          "points": 1
        }
      },
      "source": [
        "BEGIN SOLUTION\n",
        "END SOLUTION\n",
        "cuda_sched_plt([\"cs_warp.dat\"], start_t=0, end_t=float(\"inf\"), start_thread=0, end_thread=float(\"inf\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "SoS",
      "language": "sos",
      "name": "sos"
    },
    "language_info": {
      "codemirror_mode": "sos",
      "file_extension": ".sos",
      "mimetype": "text/x-sos",
      "name": "sos",
      "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
      "pygments_lexer": "sos"
    },
    "sos": {
      "kernels": [
        [
          "Bash",
          "bash",
          "bash",
          "",
          "shell"
        ],
        [
          "C",
          "c_kernel",
          "c",
          "",
          ""
        ],
        [
          "Go",
          "gophernotes",
          "go",
          "",
          ""
        ],
        [
          "Julia 1.10.2",
          "julia-1.10",
          "julia",
          "",
          ""
        ],
        [
          "OCaml default",
          "ocaml-jupyter",
          "OCaml",
          "",
          "text/x-ocaml"
        ],
        [
          "Python 3 (ipykernel)",
          "python3",
          "python3",
          "",
          {
            "name": "ipython",
            "version": 3
          }
        ],
        [
          "Rust",
          "rust",
          "rust",
          "",
          ""
        ]
      ],
      "panel": {
        "displayed": true,
        "height": 0
      },
      "version": "0.23.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}