<!--- md --->
#* Cost of Data Access (Caches and Memory Performance)

# Introduction

<!--- end md --->

<!--- md --->
# Compilers

## Set up NVIDIA CUDA and HPC SDK

Execute this before you use NVIDIA HPC SDK
<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.9/compilers/bin:$PATH
export PATH=/usr/local/cuda/bin:$PATH
<!--- end code --->

<!--- md --->
* Check if it works
  * make sure the full path of nvcc is shown as `/usr/local/...`, not `/opt/nvidia/...`
<!--- end md --->

<!--- code w kernel=bash --->
which nvcc
which nvc
which nvc++
nvcc --version
nvc --version
<!--- end code --->

<!--- md --->
## Set up LLVM

Execute this before you use LLVM

<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/home/share/llvm/bin:$PATH
export LD_LIBRARY_PATH=/home/share/llvm/lib:/home/share/llvm/lib/x86_64-unknown-linux-gnu:$LD_LIBRARY_PATH
<!--- end code --->

<!--- md --->
Check if it works (check if full paths of gcc/g++ are shown)

* <font color="red">For reasons I describe below, I do not recommend clang++ for the exercise in this notebook</font>
<!--- end md --->

<!--- code w kernel=bash --->
which clang
which clang++
<!--- end code --->

<!--- md --->
## GCC

<!--- end md --->

<!--- md --->
Check if it works (check if full paths of nvc/nvc++ are shown)
<!--- end md --->

<!--- code w kernel=bash --->
which gcc
which g++
<!--- end code --->

<!--- md --->
# Measuring Latency

* We do a simple experiment to measure the _latency_ of data access when the data comes from various levels of caches
* We want to execute many ($n$) load instructions each of which is _dependent_ on the previous load instruction, measure the execution time ($T$), and divide it by $n$ (to get $T/n$)
* To make a load instruction dependent on the previous load instruction, we determine which address it accesses based on the result of the previous load instruction, like this
```
k = 0;
for (i = 0; i < n; i++) {
  k = a[k];
}
```
* A similar access behavior happens when the processor _chases pointers_ like this
```
p = ...;
for (i = 0; i < n; i++) {
  p = p->next;
}
```
so we call this kind of code _pointer chasing_ code, although we do not explicitly use pointers ($k$ serves as a "pseudo pointer" that specifies the next element that should be accessed)
* We change the size of array $a$ and make sure the above loop repeatedly touches every element of $a$
* Here is an example of $a$ (with 16 elements) and (part of) the resulting access order (`a[0] -> a[3] -> a[14] -> a[10] -> a[7] -> a[15] -> a[1] -> ... -> a[4] -> a[0] -> ..`)
* confirm that the resulting chain comes back to `a[0]` after accessing _all_ 16 elements of the array $a$

<img src="svg/latency_measurement_L1.svg" />

* We also make sure the resulting access order is essentially random, to avoid the effect of prefetching or any smartness the processor might implement to run the above loop faster than an iteration / latency of the load instruction.

<!--- end md --->

<!--- md --->
* We use only a single thread for now,
* Although it is meant to be a single-thread experiment, we still use OpenMP so that it can run on GPUs too (with a single source code)
* For readability, we split the program into two files
* Here is the main function
<!--- end md --->

<!--- code w kernel=python --->
%%writefile main.cc
<!--- exec-include ./mk_version.py -DVER=\"omp\" -DDBG=0 nb/source/pd07_mem_ext/include/main.cc --->
<!--- end code --->

<!--- md --->
* Here is the core part of the program that accesses the array
<!--- end md --->

<!--- code w kernel=python --->
%%writefile latency.cc
<!--- exec-include ./mk_version.py -DVER=\"omp\" -DDBG=0 nb/source/pd07_mem_ext/include/latency.cc --->
<!--- end code --->

<!--- md --->
* Compile them together
<!--- end md --->

<!--- code w kernel=bash points=1 --->
#clang++ -DDBG=0 -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 -o latency latency.cc main.cc
nvc++   -Wall -mavx512f -mfma -mp=gpu -cuda -o latency latency.cc main.cc
<!--- end code --->

<!--- md --->
* run it on a CPU with a single thread (remember `OMP_TARGET_OFFLOAD=DISABLED` disables GPU execution)
* let's run it with $m = 2^{24}$ elements $= 8 \times m = $ 128MB, sufficiently above its last level cache (57MB)
* the parameter $n$ below specifies how many accesses we perform (`n` below)
```
k = 0;
for (i = 0; i < n; i++) {
  k = a[k];
}
```

* the following command will take something like 15 seconds (be patient)
<!--- end md --->

<!--- code w kernel=bash points=1 --->
# single thread execution on CPU
# most data accesses will miss all caches
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
./latency --n-elements ${m} --min-accesses ${n}
<!--- end code --->

<!--- md --->
* look at the number shown as
```
latency_per_elem : XYZ nsec/elem
```
which gives you the latecy imposed by _main memory_ access (when the accesses misses caches at any level)

* observe the latency to main memory is very large (e.g., on 2.4 GHz processor, 1 nanosecond = 2.4 cycles, thus 80 nanoseconds is as large as 200 cycles) compared to typical latency of simple arithmetic (a few cycles)

* now look at the latency of L1 (faster/smalest level) cache, buy making $a$ smaller than the L1 cache size (64KB)

* we set $m = 2^{12}$ so that $a$ occupies 32KB, sufficiently smaller than L1 cache

* note that we set $n$ to the same value with before, so this program executes exactly the same number of iterations, with the only difference being how large area the array $a$ spans
<!--- end md --->

<!--- code w kernel=bash points=1 --->
# single thread execution on CPU
# most data accesses will hit L1 cache
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 12))
export n=$((1 << 27))
./latency --n-elements ${m} --min-accesses ${n}
<!--- end code --->

<!--- md --->
* now run the same program on a GPU, again with a single (CUDA) thread (remember `OMP_TARGET_OFFLOAD=MANDATORY` makes sure the target region runs on GPU)
* all other parameters are set equal to CPU
<!--- end md --->

<!--- code w kernel=bash points=1 --->
# single thread execution on GPU
# most data accesses will miss all caches
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
./latency --n-elements ${m} --min-accesses ${n}
<!--- end code --->

<!--- md --->
* observe the latency difference between CPU and GPU
* GPU has a few times larger latency to the main memory

* let's see what happens for array $a$ smaller than the L1 cache (192KB)
* to make a comarison to CPU, we set $m$ to the same value as the CPU experiment ($2^{12}$)
<!--- end md --->

<!--- code w kernel=bash points=1 --->
# single thread execution on GPU
# most data accesses will hit L1 cache
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 12))
export n=$((1 << 27))
./latency --n-elements ${m} --min-accesses ${n}
<!--- end code --->

<!--- md --->
* it is interesting to see the huge difference in L1 cache latency between CPU and GPU
* GPU imposes several dezens of nanoseconds even when an access hits the fastest cache, whereas the L1 latency of CPU caches is as small as a few nanoseconds (a few cycles)
<!--- end md --->

<!--- md --->
# Plotting the Latencies
<!--- end md --->

<!--- md --->
* let's see how the latency is affected by the cache level data are coming from
* to see this, we plot the relationship between the size of $a$ and the latency per access
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export min_m=1000
export max_m=$((1 << 25))
export n=$((1 << 27))

m=${min_m}
while [ ${m} -lt ${max_m} ]; do
    echo "==== m=${m} ===="
    ./latency --n-elements ${m} --min-accesses ${n}
    m=$((m * 5 / 4))
done | tee cpu.txt
echo "done"
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_latency(["cpu.txt"])
<!--- end code --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export min_m=1000
export max_m=$((1 << 25))
export n=$((1 << 27))

m=${min_m}
while [ ${m} -lt ${max_m} ]; do
    echo "==== m=${m} ===="
    ./latency --n-elements ${m} --min-accesses ${n}
    m=$((m * 5 / 4))
done | tee gpu.txt
echo "done"
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_latency(["gpu.txt"])
<!--- end code --->

<!--- md --->
* Compare the CPU and the GPU
<!--- end md --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_latency(["cpu.txt", "gpu.txt"])
<!--- end code --->

<!--- md --->
# Increasing the Bandwidth

* The bandwidth reported above is

$$ \frac{\mbox{sizeof(long)}}{\mbox{latency per element}} $$

and it is essentially the reciprocal (inverse) of the latency

* When we look at the values when $a$ is much larger than the last level cache (so data are all coming from the main memory), the observed value is very small (e.g., $\approx 0.08$ GB/sec on CPU and $\approx 0.02$ GB/sec on GPU)
* They are much smaller than the advertised hardware bandwidth ($>$ 50 GB/sec for the CPU we are using and $\approx$ 1.5 TB/sec for the A100 GPU we are using)
* Just as we cannot reduce the latency of arithmetic, there is no way to reduce the latency between the main memory and the processor
* We can only increase the _bandwidth_ (the amount of data we can move per unit time) by _increasing the parallelism_

* There are multiple ways to do that
  * On CPUs, it is essential to exploit instruction level parallelism _in a single thread_, which can be done by performing several loops like this for different regions of $a$ (different `start` index below)
```
k = start;
for (i = 0; i < n; i++) {
  k = a[k];
}
```
  * On GPUs, principle is the same, but parallelism can be easily and most naturally extracted by having many CUDA threads perfoming a loop like the above

* Either way, we make multiple chains of pointers, each of which covers a disjoint region of the entire array
* Here is a simple example having _two_ 8-element chains of pointers
* Confirm that the chain starting from $a[1]$ makes another 8-element chain (a[1], a[8], a[9], ...)

<img src="svg/latency_measurement_L2.svg" />

<!--- end md --->

<!--- md --->
## Traversing Multiple Pointer Chains by A Single Thread

* We can chase two chains simultaneously by essentially doing something like this
```
k0 = 0;
k1 = 1;
for (i = 0; i < n; i++) {
  k0 = a[k0];
  k1 = a[k1];
}
```

* The code generalizes this idea so that we can chase an arbitrary number of ($C$) chains simultaneously
<!--- end md --->

<!--- code w kernel=python --->
%%writefile latency_ilp.cc
<!--- exec-include ./mk_version.py -DVER=\"omp_ilp\" -DDBG=0 nb/source/pd07_mem_ext/include/latency.cc --->
<!--- end code --->

<!--- md --->
* This code uses a variable-length array to have a number of ($C$) variables which is determined by command line
* It is not supported by `nvc++`, 
* `clang++` supports this, but only on CPUs
* Here is the trick to avoid variable length arrays by using templates up to a constant number
<!--- end md --->

<!--- code w kernel=python --->
%%writefile latency_ilp_c.cc
<!--- exec-include ./mk_version.py -DVER=\"omp_ilp_c\" -DDBG=0 nb/source/pd07_mem_ext/include/latency.cc --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
#clang++ -DDBG=0 -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 -o latency_ilp_c latency_ilp_c.cc main.cc
nvc++   -Wall -mavx512f -mfma -mp=gpu -cuda -o latency_ilp_c latency_ilp_c.cc main.cc
<!--- end code --->

<!--- md --->
* To explore the effect of chasing multiple pointer chains simultaneously, the program has two parameters
  * `--n-cycles` : the number of disjoint chain of pointers in the array
  * `--n-conc-cycles` : the number of chains of pointers we traverse simultaneously (2 for the code shown just above)
* Obviously we need to set the former as large as the latter
* Below we simply set them to the same number ($C$)

* First let's set $C$ to 1
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=1
./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}
<!--- end code --->

<!--- md --->
* Observe that this case shows a similar performance with the previous version
* Now let's set $C$ to 2 and see what happens
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=2
./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}
<!--- end code --->

<!--- md --->
* Observe that `total_accesses` is the same and the execution time is almost halved (i.e., the bandwidth (`bw`) almost doubled)
* Play with larger values of `C`
<!--- end md --->

<!--- md --->
* Make sure that this is not an unintended side effect of changing the way cycles are formed, by setting `--n-cycles 2` and comparing the two cases `--n-conc-cycles 1` and `--n-conc-cycles 2`
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=1
./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles 2 --n-conc-cycles ${C}
<!--- end code --->

<!--- md --->
## Plotting C vs. Bandwidth (CPU)
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=DISABLED
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
m=$((1 << 24))
n=$((1 << 27))

for C in 1 2 3 4 6 8 10 12 14 16 ; do
    echo "==== C=${C} ===="
    ./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}
done | tee cpu_bw.txt
echo "done"
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw(["cpu_bw.txt"])
<!--- end code --->

<!--- md --->
## Plotting C vs. Bandwidth (GPU)

* Let's see whether a similar thing happens on the GPU
* Set the value of `C` to 1, 2, 3, ... and see the effect
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=1
./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}
<!--- end code --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export OMP_NUM_THREADS=1
m=$((1 << 24))
n=$((1 << 27))

for C in 1 2 3 4 6 8 10 12 14 16 ; do
    echo "==== C=${C} ===="
    ./latency_ilp_c --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --n-conc-cycles ${C}
done | tee gpu_bw.txt
echo "done"
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw(["gpu_bw.txt"])
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw(["cpu_bw.txt", "gpu_bw.txt"])
<!--- end code --->


<!--- md --->
## Traversing Multiple Pointer Chains by Multiple Threads on GPU (OpenMP)
<!--- end md --->

<!--- md --->
* For GPU, a much more straightforward way to increase the bandwidth is, of course, increasing the number of threads
* We simply set the number of cycles (`--n-cycles`) and the number of threads to the same number
* It lets each thread follow a single pointer chain
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=1
export OMP_NUM_THREADS=${C}
./latency --n-elements ${m} --min-accesses ${n} --n-cycles ${C}
<!--- end code --->

<!--- md --->
* Increase C to 32, 64, ... (a multiple of 32) and see how far you can go

* Let's visualize how the bandwidth increases with the number of threads
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
m=$((1 << 24))
n=$((1 << 27))

for C in 32 64 96 128 192 256 384 512 768 1024 ; do
    echo "==== C=${C} ===="
    OMP_NUM_THREADS=${C} ./latency --n-elements ${m} --min-accesses ${n} --n-cycles ${C}
done | tee gpu_bw_threads.txt
echo "done"
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw_threads(["gpu_bw_threads.txt"])
<!--- end code --->

<!--- md --->
* <font color="red">NOTE:</font> I found a number of issues and mysteries with `clang++` and `nvc++` in this experiment
* `clang++`
  * performance is very poor (the bandwidth does not increase almost at all when increasing the number of threads)
  * I haven't had the time to look into it
* `nvc++`
  * bandwidth scales much better than clang++
  * but when I give `-O2` option or above in the command line, the compilation succeeds but when executing the commnad, it fails with the following error

```
$ ./latency --n-elements 16777216 --min-accesses 134217728 --n-cycles 32
n_elements : 16777216
sz : 134217728 bytes
n_cycles : 32
len_cycle : 524288
n_accesses_per_cycle : 4194304
total_accesses : 134217728
n_conc_cycles : 1
coalese_size : 1
seed : 123456789012345
Module function not found, error 500
Accelerator Fatal Error: Failed to find device function 'nvkernel__Z6cyclesPlllS_llllPi_F1L83_2'! File was compiled with: -gpu=cc80
Rebuild this file with -gpu=cc80 to use NVIDIA Tesla GPU 0
 File: /home/pd/parallel-distributed-programming/jupyter/nb/source/pd07_mem_ext/include/ver/main_omp.cc
 Function: _Z11make_cyclesPlS_lllll:101
 Line: 101
```

* The best workaround for now is to **use nvc++ with `-O1` or below**
* Or use CUDA, which I demonstarte below
<!--- end md --->

<!--- md --->
## Traversing Multiple Pointer Chains by Multiple Threads on GPU (CUDA)

* This would be unnecessary if OpenMP result were more clear-cut

* The main file
<!--- end md --->

<!--- code w kernel=python --->
%%writefile main_cuda.cc
<!--- exec-include ./mk_version.py -DVER=\"cuda\" -DDBG=0 nb/source/pd07_mem_ext/include/main.cc --->
<!--- end code --->

<!--- md --->
* The core part of the program that accesses the array
<!--- end md --->

<!--- code w kernel=python --->
%%writefile latency_cuda.cc
<!--- exec-include ./mk_version.py -DVER=\"cuda\" -DDBG=0 nb/source/pd07_mem_ext/include/latency.cc --->
<!--- end code --->

<!--- md --->
* Compile them together
<!--- end md --->

<!--- code w kernel=bash points=1 --->
nvcc -DDBG=0 -O4 -x cu -o latency_cuda latency_cuda.cc main_cuda.cc
<!--- end code --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
m=$((1 << 24))
n=$((1 << 27))

for C in 32 64 96 128 192 256 384 512 768 1024 ; do
    echo "==== C=${C} ===="
    OMP_NUM_THREADS=${C} ./latency_cuda --n-elements ${m} --min-accesses ${n} --n-cycles ${C}
done | tee gpu_bw_threads.txt
echo "done"
<!--- end code --->

<!--- md --->
* Visualize
<!--- end md --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw_threads(["gpu_bw_threads.txt"])
<!--- end code --->

<!--- md --->
## Coalesing Global Memory Accesses

* In NVIDIA GPUs, global memory accesses get faster when all threads witin a warp access a consecutive addresses
* This is the case, for example, when thread 0 in a warp accesses a[0], thread 1 a[1], threads 2 a[2], and so on (contiguous accesses)
* This is not the case, on the other hand, when thread 0 accesses a[0], thread 1 a[50], thread 2 a[28], and so on (non-contiguous or scattered accesses)
* This is natural if you consider all threads within a warp execute the same instruction and the processor has a cache line larger than a single word; if those threads access a contiguous addresses, data are in a single or a small number of cache lines that are brought to the processor with a single memory transaction
  * In NVIDIA GPU, it is called _coalesing_
* The experiment above was performing scattered accesses; different chains connect totally unrelated elements
* We can arrange chains in such a way that a certain number of ($= D$) chains starting from a consecutive location always point to a consecutive location to see the effect of coalesing
* Here is an example of two coalesed chains (i.e., $D = 2$)

<img src="svg/latency_measurement_L3.svg" />

<!--- end md --->

<!--- md --->
* You can specify `--coalese-size` option to set the number of consecutive elements that are always coalesed
* For example, the above picture can be realized by `--n-cycles 2 --coalese-size 2`
* Change the coalese size ($D$) in the command below and see what happens
* Note: $D$ must divide $C$, so when $C = 64$, you can set $D$ only to a power of two up to 64

* For clearer result, I am using CUDA program, but you can get a similar result with OpenMP, as long as it is compiled by nvc++, not clang++
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
export m=$((1 << 24))
export n=$((1 << 27))
export C=64
export D=1
export OMP_NUM_THREADS=${C}
./latency_cuda --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --coalese-size ${D}
<!--- end code --->

<!--- md --->
* Let's visualize it
<!--- end md --->

<!--- code w kernel=bash points=1 --->
export OMP_TARGET_OFFLOAD=MANDATORY
export OMP_NUM_TEAMS=1
m=$((1 << 24))
n=$((1 << 27))

for D in 1 2 8 32; do
    for C in 32 64 96 128 192 256 384 512 768 1024 ; do
        echo "==== C=${C} ===="
        OMP_NUM_THREADS=${C} ./latency_cuda --n-elements ${m} --min-accesses ${n} --n-cycles ${C} --coalese-size ${D}
    done | tee gpu_bw_threads_coalese_${D}.txt
done
<!--- end code --->

<!--- code w kernel=python --->
import vis_mem
vis_mem.vis_bw_threads([f"gpu_bw_threads_coalese_{D}.txt" for D in [1,2,8,32]])
<!--- end code --->

