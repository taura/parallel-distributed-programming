<!--- md --->

# GPU Scheduling 

<!--- end md --->

<!--- md w --->

Enter your name and student ID.

 * Name:
 * Student ID:

<!--- end md --->


<!--- md --->

This notebook demonstrates how NVIDIA GPU is scheduling threads.

# Compilers

* This exercise is CUDA-specific, so we use NVIDIA CUDA

* Execute this before you use CUDA
<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/usr/local/cuda/bin:$PATH
<!--- end code --->

<!--- md --->
* Check if it works (check if full paths of nvcc are shown)
* We do not recommend nvc/nvc++ for this exercise, but you might give them a try if you like
<!--- end md --->

<!--- code w kernel=bash --->
which nvcc
which nvc
which nvc++
<!--- end code --->

<!--- md --->
# Check host and GPU

* Check if you are using the right host, tauleg000, <font color="red">not taulec</font>
<!--- end md --->

<!--- code w kernel=bash --->
hostname
hostname | grep tauleg || echo "Oh, you are not on the right host, access https://tauleg000.zapto.org/ instead"
<!--- end code --->

<!--- md --->
* Check if GPU is alive by nvidia-smi
* Do `nvidia-smi --help` or see manual (`man nvidia-smi` on terminal) for more info
<!--- end md --->

<!--- code w kernel=bash --->
nvidia-smi
<!--- end code --->

<!--- md --->

# Basics

When you call a kernel (function f) with
```
f<<<nb,bs>>>();
```
it creates (_nb * bs_) CUDA threads.

More precisely, it creates _nb_ thread blocks, each of which has _bs_ CUDA threads.

The following code is a tool to record how threads are executed on GPU.

It creates many threads repeating a trivial (useless) computation x = a * x + b many times.
Each thread occasionally records the clock to record when and where these threads progress over time.

Specifically,

```
./cuda_sched_rec NTHREADS THREAD_BLOCK_SZ N M 
```

creates approximately NTHREADS threads, with THREAD_BLOCK_SZ threads in each thread block (the number of threads is not exactly NTHREADS when it is not a multiple of THREAD_BLOCK_SZ).

* Each thread repeats x = A x + B, (N * M) times.
* Each thread records clock N times (every M iterations).

* At the end of execution, it dumps the results in the following format for each line.

```
thread=<idx> x=<ans> sm0=<starting SM> sm1=<ending SM> t0 t1 t2 ... t_{n-1}
```

<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile cuda_sched_rec.cu
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd04_cuda_sched_vis/include/cuda_sched_rec.cu --->
<!--- end code --->

<!--- md --->
Read the code carefully and understand what it is doing.  
<!--- end md --->

<!--- code w kernel=bash points=1 --->
nvcc --generate-code arch=compute_80,code=sm_80 -o cuda_sched_rec cuda_sched_rec.cu
<!--- end code --->

<!--- code w kernel=bash points=1 --->
./cuda_sched_rec 64 32 10 100 | head -10
<!--- end code --->

<!--- md --->
# Visualization

* The following python code parses and visualizes the output of cuda_sched_rec.
* The code is shown below for your information; you don't have to understand how it works.
<!--- end md --->

<!--- code w kernel=python points=1 --->
<!--- include nb/source/pd04_cuda_sched_vis/include/cuda_sched_vis.py --->
<!--- end code --->

<!--- md --->
Let's visualize a few configurations.

## one thread
<!--- end md --->

<!--- code w kernel=bash points=1 --->
./cuda_sched_rec 1 1 100 1000 > cs_1_1.dat
<!--- end code --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_1_1.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->

<!--- md --->
* you can change `start_t` and `end_t` to zoom into a narrower time interval and change `start_thread` and `end_thread` to zoom into a range of threads
* or, you can open `sched.svg` generated along with the visualization and magnify anywhere you want to look into, either by the browser or any SVG viewer on your PC
<!--- end md --->

<!--- md --->
## many threads with 1 thread/block

* play with changing N to other values
<!--- end md --->

<!--- code w kernel=bash points=1 --->
N=150
./cuda_sched_rec ${N} 1 100 1000 > cs_N_1.dat
<!--- end code --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_N_1.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->

<!--- md --->
* Increase N and observe when the execution time (the time at the right end of the graph) starts to increase.
* Even in that case, all N threads appear to be executing simultaneously (not one after another).
* That is, _hardware_ interleaves execution of these threads, rapidly switching from one to another.
<!--- end md --->

<!--- md --->
## many threads in 1 thread block

* play with changing N to other values
<!--- end md --->

<!--- code w kernel=bash points=1 --->
N=150
./cuda_sched_rec ${N} ${N} 100 1000 > cs_N_N.dat
<!--- end code --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_N_N.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->

<!--- md --->
* Observe that they are always executed on the same SM. You are not utilizing multiple SMs at all.

* There is a hardwired limit on the number of threads per block. Try to find it and then confirm it with Google.

* When increasing N, observe when the execution time starts to increase. Why do you think it doesn't immediately increase with N&gt;1?

* With a modest value of N (say 100), zoom in at either end of the execution and observe whether there is _any_ difference on when they started or finished execution.  If you look carefully, you will notice that a number of consecutive threads start and end _exactly the same clock_.  Those threads are called a _warp_ and they share an instruction pointer.  It is very analogous to SIMD instruction found in CPUs that apply the same operation on multiple operands.  Guess the number of threads of a warp from the experimental results and confirm it by Google.
<!--- end md --->

<!--- md --->
## many threads in many threads/block

* play with changing N and B to other values
<!--- end md --->

<!--- code w kernel=bash points=1 --->
N=150
B=64
./cuda_sched_rec ${N} ${B} 100 1000 > cs_N_B.dat
<!--- end code --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_N_B.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->

<!--- md --->
* Try to find the maximum number of threads that does not increase the execution time.
<!--- end md --->

<!--- md --->
# Thread blocks

A thread block is the unit of dispatching to a streaming multiprocessor (SM), which is like a physical core of a CPU.  Threads within a thread block are always dispatched together to the same SM and once dispatched stay on the same SM until finished.

* see [CUDA C++ Programming Guide: A Scalable Programming Model](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#scalable-programming-model)

An SM is a highly multithreaded processor, which can accommodate many threads at the same time and interleave them  by hardware.  For example, it can easily hold, say, 500 threads and interleave their execution without involving software.  In terms of hardware capability, it is somewhat similar to simultaneous multithreading (SIMT) of CPUs.  The degree of multithreading is very different, however; Intel CPUs normally support only two hardware threads (virtual cores) on each physical core.  Moreover, software (either operating system or user-level software) needs to designate which virtual core you want to run a thread on.  In a sense, CPU exposes each virtual core as a single-threaded machine.  If you put more than one OpenMP (OS-level) thread on the same virtual core, software should switch between them from time to time.  A streaming multiprocessor of a GPU, in contrast, is a machine that literally takes many threads and concurrently executes them by hardware.  Determining the SM a thread block executes on is done by hardware.

How many thread blocks are scheduled on an SM at the same time?  It depends; it depends on how much "resources" a single thread block requires.  Here, "resources" mean two things.

1. registers
1. shared memory (see below)


_Registers_ are used for holding local variables and intermediate results of computation.  How many registers a thread block uses is not something you can reliably determine by looking at your code; it depends on the code generated by the compiler.  You can know it by passing `-Xptxas -v` to nvcc and looking at the compiler message.

_Shared memory_ is a scratch-pad memory only shared within a single thread block.  Physically, you can consider it to be a small fast memory attached to each SM.  The name "shared memory" is clearly a misnomer; ordinary memory you get by `cudaMalloc` _is_ shared by all threads (called "global memory").  In contrast, shared memory is, contrary to its name, shared only among threads within a single thread block.  "Local memory" (as opposed to global memory) would have been a better name for it, IMO.

Both registers and shared memory for a thread block are kept on physical registers/memory of an SM throughout the lifetime of the thread block.  Thus, accommodating a larger number of thread blocks at the same time requires a proportionally larger amount of registers/shared memory, which is subject to the physical resource limit of an SM.

Each SM has the following physical resources.

|       | registers      |  shared memory  |
|-------|----------------|-----------------|
|Pascal | 32 bit x 65536 |  64KB           |
|Volta  | 32 bit x 65536 |  up to 96KB (*) |
|Ampere | 32 bit x 65536 |  up to 163KB    |

(*) configurable subject to L1 cache + shared memory <= 128KB and shared memory <= 96KB

* [Pascal Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/pascal-tuning-guide/index.html#sm-occupancy)
* [Volta Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/volta-tuning-guide/index.html#sm-occupancy)
* [NVIDIA Ampere GPU Architecture Tuning Guide: Occupancy](https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html#sm-occupancy)

By default, a thread does not use shared memory at all.

Let's observe how many registers a thread uses.
<!--- end md --->

<!--- code w kernel=bash points=1 --->
nvcc --generate-code arch=compute_80,code=sm_80 -Xptxas -v -o cuda_sched_rec cuda_sched_rec.cu
<!--- end code --->

<!--- md --->
Since the computation is very simple, register usage will not be a limiting factor for this computation.
Also, since it does not use shared memory at all, it won't be a limiting factor either.
Only the hardwired limit is the limiting factor.
<!--- end md --->

<!--- md --->
# Shared memory

* Let's use shared memory to observe how it affects the number of thread blocks simultaneously executed.
You specify the size of shared memory per thread block via the third parameter of kernel call, like this.

```
f<<<nb,bs,S>>>();
```

The above kernel launch statement specifies that $S$ bytes of shared memory should be allocated to _each thread block_.  Each SM can therefore execute only up to (SHARED_MEMORY_SIZE_PER_SM / $S$) thread blocks simultaneously.

You can get a pointer to the part of the shared memory allocated to each thread via the following strange syntax within your kernel function, though it is not necessary in our current experiment.

```
extern __shared__ T shmem[];
```

With that, `shmem` points to the start of the shared memory for the thread block.  The name can be arbitrary.

`cuda_sched_rec.cu` is already written to take the size of the shared memory per thread block as a parameter.

Let's allocate 32KB for each thread block; then, on Ampere, only up to three thread blocks (163KB/32KB) can be executed simultaneously.

The following creates 100 thread blocks (in order to avoid creating too many threads, it will set the thread per block to an unusual value of one).
<!--- end md --->

<!--- code w kernel=bash points=1 --->
N=150
S=$((32 * 1024))
./cuda_sched_rec ${N} 1 100 1000 ${S} > cs_N_1_S.dat
<!--- end code --->

<!--- md --->
Before visualizing it, imagine what it is like.
<!--- end md --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_N_1_S.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->

<!--- md --->
* Play with changing $N$ above; predict when thread blocks start executing not simultaneously (one after another) and confirm it by the experiment (hint: Ampere has 108 streaming multiprocessors).
* Change $S$ and see how it affects the above threshold value.
<!--- end md --->

<!--- md --->
# Warp

* Consecutively numbered 32 threads within a thread block makes a _warp_ and they can execute only one same instruction at a time.
* That is, it's not possible, within a single cycle, for some threads to execute an instruction A while others in the same warp execute another instruction B.  All the GPU can do is simply to keep some threads from executing instructions that they should not execute.
* A typical example is an "if" statement. e.g.,
```
if (thread_idx % 2 == 0) {
  A;
} else {
  B;
}
```
If there are _any_ thread executing A and _any_ thread executing B within a warp, the time the warp takes is the time to execute A _plus_ the time to execute B.
* An important performance implication is you'd better not have threads branching differently within the same warp.

* Change the following code as follows.
  * it takes an additional command line parameter D
  * each thread executes the loop
```
      for (long j = 0; j < m; j++) {
        x = a * x + b;
      }
```
when and only when (idx / D) is an odd number.
  * for example, if D is 1, then all even-numbered threads execute the loop and all odd-numbered threads do not execute it
  * if D is 32, for example, (idx / D) is essentially the "warp index"; even-numbered warps execute the loop and odd-numbered warps skip it
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile cuda_sched_rec_warp.cu
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd04_cuda_sched_vis/include/cuda_sched_rec.cu --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
nvcc --generate-code arch=compute_80,code=sm_80 -o cuda_sched_rec_warp cuda_sched_rec_warp.cu
<!--- end code --->

<!--- md --->
Execute the code with various D's (and perhaps other parameters) to visualize the effect of warps and its performance implication
<!--- end md --->

<!--- code w kernel=bash points=1 --->
N=256
./cuda_sched_rec_warp ${N} 32 100 1000 1 > cs_warp.dat
<!--- end code --->

<!--- code w kernel=python points=1 --->
cuda_sched_plt(["cs_warp.dat"], start_t=0, end_t=float("inf"), start_thread=0, end_thread=float("inf"))
<!--- end code --->
