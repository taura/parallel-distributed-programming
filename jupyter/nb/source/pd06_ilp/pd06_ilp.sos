<!--- md --->
#* Instruction Level Parallelism

# Introduction

* we have learned that CPU has multicore and SIMD parallelism
* the last dimension of parallelism is _Instruction Level Parallelism (ILP)_, the ability to execute many instructions <font color=red>of a single thread</font> concurrently (i.e., execution of many instructions overlap in time)
* in contrast, GPU does not aggressively try to extract ILP from a single thread; parallelism mostly comes from simultaneously executing many threads
* illuminating the difference between the two is both instructive and practically important, especially for optimizing CPU code
<!--- end md --->

<!--- md --->
# Compilers

## Set up NVIDIA HPC SDK

Execute this before you use NVIDIA HPC SDK
<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.9/compilers/bin:$PATH
<!--- end code --->

<!--- md --->
Check if it works (check if full paths of nvc/nvc++ are shown)
<!--- end md --->

<!--- code w kernel=bash --->
which nvc
which nvc++
<!--- end code --->

<!--- md --->
## Set up LLVM

Execute this before you use LLVM

<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/home/share/llvm/bin:$PATH
export LD_LIBRARY_PATH=/home/share/llvm/lib:/home/share/llvm/lib/x86_64-unknown-linux-gnu:$LD_LIBRARY_PATH
<!--- end code --->

<!--- md --->
Check if it works (check if full paths of gcc/g++ are shown)
<!--- end md --->

<!--- code w kernel=bash --->
which clang
which clang++
<!--- end code --->

<!--- md --->
## GCC

<!--- end md --->

<!--- md --->
Check if it works (check if full paths of nvc/nvc++ are shown)
<!--- end md --->

<!--- code w kernel=bash --->
which gcc
which g++
<!--- end code --->

<!--- md --->
# CPU without ILP

* this is an experiment very similar to what we did on GPUs
* each thread repeats x = a * x + b many times and occasionally record time
* although the primary focus is a single-thread performance, the program is written with OpenMP so it can be executed on multicore CPUs and GPUs
* the compilation option below enables execution on both GPU and CPU
<!--- end md --->

<!--- code w kernel=python --->
%%writefile no_ilp.cc
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd06_ilp/include/ilp_rec.cc --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang++ -Wall -O3 -mavx512f -mfma -fopenmp no_ilp.cc -o no_ilp
#nvc++   -Wall -O3 -mavx512f -mfma -mp=multicore -cuda no_ilp.cc -o no_ilp
#g++     -Wall -O3 -mavx512f -mfma -fopenmp no_ilp.cc -o no_ilp
<!--- end code --->

<!--- md --->
* run it on a single core
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./no_ilp 1 100 1000000 > a.dat
cat a.dat
<!--- end code --->

<!--- md --->
* it shows cycles per iteration (of the innermost loop) by `cycles_per_iter=xxxx`, which can be extracted by
<!--- end md --->

<!--- code w kernel=bash points=1 --->
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* this is essentially the _latency_ of an FMA instruction 
* according to [Intel intrinsics guide](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html), the latency of FMA instruction is 4, but we observe a slightly lower value
* this is due to dynamic frequency scaling (essentially, a core boosts its frequency when the total load is low; the `rdtsc` instruction we use to get the clock is a clock that runs at a constant speed regardless of the actual processor frequency)
<!--- end md --->

<!--- md --->
* let's witness instruction sequence 
* identify the instruction sequence corresponding to the innermost loop
<!--- end md --->

<!--- code w kernel=bash points=1 --->
clang++ -Wall -O3 -mavx512f -mfma -fopenmp no_ilp.cc -S
#nvc++   -Wall -O3 -mavx512f -mfma -mp=multicore -cuda no_ilp.cc -S
#g++     -Wall -O3 -mavx512f -mfma -fopenmp no_ilp.cc -S
<!--- end code --->

<!--- code w kernel=bash points=1 --->
cat no_ilp.s
<!--- end code --->

<!--- md --->
# Double ILP on CPU

* an ILP increases when there are multiple instructions that are _independent_
* no processor can execute the following sequence faster than 4 cycles / FMA, because each instruction has to wait for the previous instruction to produce its result (i.e., depends on the previous instruction)
```
x = a * x + b
x = a * x + b
x = a * x + b
  ...
```
* what a CPU _can_ do is to execute something like the following as fast as 4 cycles / (2 FMAs)
```
x0 = a * x0 + b
x1 = a * x1 + b
x0 = a * x0 + b
x1 = a * x1 + b
x0 = a * x0 + b
x1 = a * x1 + b
  ...
```
* this is possible because instructions working on x0 do not depend on those working on x1 and vice versa
<!--- end md --->

<!--- code w kernel=python --->
%%writefile ilp2.cc
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd06_ilp/include/ilp_rec.cc --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang++ -Wall -O3 -mavx512f -mfma -fopenmp ilp2.cc -o ilp2
#nvc++   -Wall -O3 -mavx512f -mfma -mp=multicore -cuda ilp2.cc -o ilp2
#g++     -Wall -O3 -mavx512f -mfma -fopenmp ilp2.cc -o ilp2
<!--- end code --->

<!--- md --->
* run it on a single core again
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp2 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* observe that the number of cycles per iteration does not change almost at all
* put differently, it increased the _throughput_, the number of operations executed per cycle (or a unit time)
<!--- end md --->

<!--- md --->
# Increase ILP further

* let's increase ILP furter
* the following code can configure the number of variables to update in the `k` loop (therefore the number of _independent_ FMA instructions) by setting a preprocessor constant `C`
<!--- end md --->

<!--- code w kernel=python --->
%%writefile ilp.cc
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd06_ilp/include/ilp_rec.cc --->
<!--- end code --->

<!--- md --->
* compile it with `C=4`
* we inhibit vectorization to make sure we witness the effect of ILP, not of SIMD instructions
<!--- end md --->

<!--- code w kernel=bash points=1 --->
clang++ -DC=4 -Wall -O3 -mavx512f -mfma -fopenmp -fno-vectorize ilp.cc -o ilp
#nvc++   -DC=4 -Wall -O3 -mavx512f -mfma -mp=multicore -Mnovect -cuda ilp.cc -o ilp
#g++     -DC=4 -Wall -O3 -mavx512f -mfma -fopenmp -fno-tree-vectorize ilp.cc -o ilp
<!--- end code --->

<!--- md --->
* and run it on a single core
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* change the `C` above and how far you can go, without noticeably increasing the cycles per iteration
* it is the processor's _throughput_ limit, determined by the number of execution units for floatint point operations (two on many CPUs)
* note: this number if the same even if it is SIMD FMA, which we will see below

* change `C` systematically and graph the result
* make sure to increase `C` to observe that the throughput initially rises and eventually plateaus at a certain point
<!--- end md --->

<!--- code w kernel=bash points=1 --->
for L in 1; do
    for M in 100 ; do
        for N in 1000000; do
            for C in 1 2     ; do  # put values of C you want to experiment with
                # choose the compiler and craft an appropriate command
                # line to produce exe file ilp_${C}
                #nvc++   -DC=${C} -Wall -O3 -mavx512f -mfma -mp=multicore -Mnovect -cuda ilp.cc -o ilp_${C} \
                #g++     -DC=${C} -Wall -O3 -mavx512f -mfma -fopenmp -fno-tree-vectorize ilp.cc -o ilp_${C} \
                clang++ -DC=${C} -Wall -O3 -mavx512f -mfma -fopenmp -fno-vectorize ilp.cc -o ilp_${C} \
                    && (echo -n "L=${L} M=${M} N=${N} C=${C} ";
                        OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp_${C} ${L} ${M} ${N} | awk '{print $5}')
            done
        done
    done
done 
<!--- end code --->

<!--- code w kernel=python --->
DATA_STRING_CPU = r"""
L=1 M=100 N=10000000 C=1 cycles_per_iter=2.128242
L=1 M=100 N=10000000 C=2 cycles_per_iter=2.126931
L=1 M=100 N=10000000 C=3 cycles_per_iter=2.125129
    ...
"""
<!--- end code --->

<!--- code w kernel=python --->
import vis_latency_throughput
vis_latency_throughput.vis(DATA_STRING_CPU)
<!--- end code --->


<!--- md --->
# GPU 

* what about GPU?
* GPU does not aggressively exploit ILP
* let's fast see the case where we have almost no ilp
* the code can be exactly the same as the CPU case; we merely have to change the compile option
<!--- end md --->

<!--- code w kernel=bash points=1 --->
clang++ -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 no_ilp.cc -o no_ilp_gpu
#nvc++   -Wall -O3 -mavx512f -mfma -mp=gpu -cuda no_ilp.cc -o no_ilp_gpu
<!--- end code --->

<!--- md --->
* and run it with a single thread
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_TARGET_OFFLOAD=MANDATORY OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./no_ilp_gpu 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* this is the _latency_ of FMA instruction on NVIDIA GPU
* it is slightly longer than in terms of the number of cycles; it is even longer in terms of absolute time (as the frequency of A100 GPU, 1.095GHz - 1.41MHz, is lower than that of Intel Xeon Platinum 8368, 2.40GHz - 3.4GHz)
<!--- end md --->

<!--- md --->
* another good news is that this same executable can actually run on CPUs too
* we merely have to disable offloading by changing the environment variable
<!--- end md --->

<!--- code w kernel=bash points=1 --->
# execute on CPU
OMP_TARGET_OFFLOAD=DISABLED OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./no_ilp_gpu 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
# GPU with ILP?

* combined, A100 GPU is about 3x slower than Xeon Platinum 8363 CPU when it comes to the latency of an FMA
* but that's not the main point
* the main point is what happens if we expose more independent FMAs to a _single thread_ of GPU
<!--- end md --->

<!--- code w kernel=bash points=1 --->
clang++ -DC=2 -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 ilp.cc -o ilp_gpu
#nvc++   -DC=2 -Wall -O3 -mavx512f -mfma -mp=gpu -cuda ilp.cc -o ilp_gpu
<!--- end code --->

<!--- md --->
* and run it with a single thread, on GPU
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_TARGET_OFFLOAD=MANDATORY OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp_gpu 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* and on CPU
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_TARGET_OFFLOAD=DISABLED OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp_gpu 1 100 1000000 > a.dat
awk '{print $5}' a.dat
<!--- end code --->

<!--- md --->
* change the value of `C` and see what happens on GPU 
<!--- end md --->

<!--- md --->
* then change `C` systematically and graph the result
* observe that latency immediately starts increasing and thus the throughput does not increase
<!--- end md --->

<!--- code w kernel=bash points=1 --->
for L in 1; do
    for M in 100 ; do
        for N in 1000000; do
            for C in 1 2    ; do  # put values of C you want to experiment with
                # choose the compiler and craft an appropriate command
                # line to produce exe file ilp_gpu_${C}
                #nvc++   -DC=${C} -Wall -O3 -mavx512f -mfma -mp=gpu -cuda ilp.cc -o ilp_gpu_${C} \
                clang++ -DC=${C} -Wall -O3 -mavx512f -mfma -fopenmp -fopenmp-targets=nvptx64 ilp.cc -o ilp_gpu_${C} \
                    && (echo -n "L=${L} M=${M} N=${N} C=${C} ";
                        OMP_TARGET_OFFLOAD=MANDATORY OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./ilp_gpu_${C} ${L} ${M} ${N} | awk '{print $5}')
            done
        done
    done
done 
<!--- end code --->

<!--- code w kernel=python --->
DATA_STRING_GPU = r"""
L=1 M=100 N=10000000 C=1 cycles_per_iter=2.128242
L=1 M=100 N=10000000 C=2 cycles_per_iter=2.126931
L=1 M=100 N=10000000 C=3 cycles_per_iter=2.125129
    ...
"""
<!--- end code --->

<!--- code w kernel=python --->
import vis_latency_throughput
vis_latency_throughput.vis(DATA_STRING_GPU)
<!--- end code --->

<!--- md --->
# CPU with SIMD x ILP

* Intel CPU is able to execute up to two _SIMD_ FMAs per cycle
* therefore, the throughput limit of a single thread is actually even higher than what you saw above

* change the following code if necessary to take advantage of SIMD and get throughput higher than the case without SIMD
* does the compiler successfully vectorize the loop without any code change?
<!--- end md --->

<!--- code w kernel=python --->
%%writefile simd_ilp.cc
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd06_ilp/include/ilp_rec.cc --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang++ -DC=16 -Wall -O3 -mavx512f -mfma -fopenmp simd_ilp.cc -o simd_ilp
#nvc++   -DC=16 -Wall -O3 -mavx512f -mfma -mp=multicore -cuda simd_ilp.cc -o simd_ilp
#g++     -DC=16 -Wall -O3 -mavx512f -mfma -fopenmp simd_ilp.cc -o simd_ilp
<!--- end code --->


<!--- md --->
* then change `C` systematically and graph the result
* make sure to increase `C` to observe that the throughput initially rises and eventually plateaus at a certain point
* see how far you can go with a single thread on CPUs
<!--- end md --->

<!--- code w kernel=bash points=1 --->
for L in 1; do
    for M in 100 ; do
        for N in 1000000; do
            for C in 1 2     ; do  # put values of C you want to experiment with
                # choose the compiler and craft an appropriate command
                # line to produce exe file simd_ilp_${C}
                #nvc++   -DC=${C} -Wall -O3 -mavx512f -mfma -mp=multicore -cuda simd_ilp.cc -o simd_ilp_${C} \
                #g++     -DC=${C} -Wall -O3 -mavx512f -mfma -fopenmp simd_ilp.cc -o simd_ilp_${C} \
                clang++ -DC=${C} -Wall -O3 -mavx512f -mfma -fopenmp simd_ilp.cc -o simd_ilp_${C} \
                    && (echo -n "L=${L} M=${M} N=${N} C=${C} ";
                        OMP_NUM_TEAMS=1 OMP_NUM_THREADS=1 OMP_PROC_BIND=true ./simd_ilp_${C} ${L} ${M} ${N} | awk '{print $5}')
            done
        done
    done
done 
<!--- end code --->

<!--- code w kernel=python --->
DATA_STRING_SIMD = r"""
L=1 M=100 N=10000000 C=1 cycles_per_iter=2.128242
L=1 M=100 N=10000000 C=2 cycles_per_iter=2.126931
L=1 M=100 N=10000000 C=3 cycles_per_iter=2.125129
    ...
"""
<!--- end code --->

<!--- code w kernel=python --->
import vis_latency_throughput
vis_latency_throughput.vis(DATA_STRING_SIMD)
<!--- end code --->

