<!--- md --->

#* OpenMP Programming Tutorial and Hands-on

<!--- end md --->

<!--- md w --->

Enter your name and student ID.

 * Name:
 * Student ID:

<!--- end md --->

<!--- md --->

# OpenMP

* <a href="http://openmp.org/" target="_blank" rel="noopener">OpenMP</a> is the de fact programming model for multicore environment
* More recently, it supports GPU offloading
* We are going to learn OpenMP both for CPU (multicore) and GPU programming
* See <a href="https://www.openmp.org/spec-html/5.0/openmp.html" target="_blank" rel="noopener">the spec</a>

<!--- end md --->

<!--- md --->
# Compilers

* We use [NVIDIA HPC SDK ver. 24.9](https://docs.nvidia.com/hpc-sdk/index.html) (`nvc` and `nvc++`) and [LLVM ver. 18.1.8](https://llvm.org/) (`clang` and `clang++`) for C/C++ compilers, as they support OpenMP GPU offloading
<!--- end md --->

<!--- md --->
## Set up NVIDIA CUDA and HPC SDK

Execute this before you use NVIDIA HPC SDK
<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/24.9/compilers/bin:$PATH
export PATH=/usr/local/cuda/bin:$PATH
<!--- end code --->

<!--- md --->
* Check if it works (check if full paths of nvc/nvc++ are shown)
* make sure
  * `which nvc` and `which nvc++` show `/opt/nvidia/...`
  * `which nvcc` shows `/usr/local/...`
<!--- end md --->

<!--- code w kernel=bash --->
which nvc
which nvc++
which nvcc
<!--- end code --->

<!--- md --->
## Set up LLVM

Execute this before you use LLVM
<!--- end md --->

<!--- code w kernel=bash --->
export PATH=/home/share/llvm/bin:$PATH
export LD_LIBRARY_PATH=/home/share/llvm/lib:/home/share/llvm/lib/x86_64-unknown-linux-gnu:$LD_LIBRARY_PATH
<!--- end code --->

<!--- md --->
Check if it works (check if full paths of clang/clang++ are shown)
<!--- end md --->

<!--- code w kernel=bash --->
which clang
which clang++
<!--- end code --->

<!--- md --->
# Compiling and running OpenMP programs

* Summary
  * `clang`/`clang++` : give `-fopenmp` option
  * `nvc`/`nvc++` : give `-mp` option
  * Set `OMP_NUM_THREADS` environment variable when running the executable
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_hello.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_hello.c --->
<!--- end code --->

<!--- md --->
* Compiling with clang
* Add `-fopenmp` option to compile OpenMP programs
* Other generally useful options
  * `-Wall` warns many suspicous code
  * `-O3` maximally optimize code for performance
<!--- end md --->

<!--- code w kernel=bash --->
clang -fopenmp omp_hello.c -o omp_hello_clang
<!--- end code --->

<!--- md --->
* Compiling with nvc
* Add `-mp` option to compile OpenMP programs
* Other generally useful options
  * `-Wall` warns many suspicous code
  * `-O4` maximally optimizes code for performance
<!--- end md --->

<!--- code w kernel=bash --->
nvc -mp omp_hello.c -o omp_hello_nvc
<!--- end code --->

<!--- md --->
* Running

* Set environment variable `OMP_NUM_THREADS` to the number of threads created by `#pragma omp parallel`
<!--- end md --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_hello_clang
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_hello_nvc
<!--- end code --->

<!--- md --->
#*P Change the number of threads

* Execute them with various numbers of threads and see what happens
<!--- end md --->

<!--- md label=ans --->
_<font color="green">Answer for trivial work omitted</font>_
<!--- end md --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_hello_clang
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_hello_nvc
<!--- end code --->

<!--- md --->
# `#pragma omp parallel`

* [#pragma omp parallel](https://www.openmp.org/spec-html/5.0/openmpse14.html#x54-800002.6) creates a _team_ of threads, each of which executes the statement below
* Note that only the statement that is right below the pragma is executed by the team of threads
* Of course, the statement can be a compound statement and/or include a function call, so each thread can actually execute arbitrary number of statements
* See [Determining the Number of Threads for a parallel Region](https://www.openmp.org/spec-html/5.0/openmpsu35.html#x55-880002.6.1) for more details on the number of threads created by `#pragma omp parallel`
<!--- end md --->

<!--- md --->
#*P Executing multiple statements by threads

* Change following program so that both "world" and "good bye" are printed as many times as the number of threads
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_hello.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_hello.c --->
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code kernel=python label=ans --->
%%writefile omp_hello_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_hello.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_hello_ans.c -o omp_hello_ans
# nvc -mp omp_hello_ans.c -o omp_hello_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=3 ./omp_hello_ans
<!--- end code --->

<!--- md --->
* Below, choose `clang` or `nvc` depending on your taste by commenting out the other one
* Below, I chose `clang` by commenting out `nvc`
<!--- end md --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_hello.c -o omp_hello
# nvc -mp omp_hello.c -o omp_hello
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_hello
<!--- end code --->

<!--- md --->
# `omp_get_num_threads()` and `omp_get_thread_num()`

* When threads are executing a statement with `#pragma omp parallel`,
  * they are said to be in a _parallel region_
  * they are called a _team_ of threads

* While a thread is executing a parallel region,
  * [omp_get_num_threads()](https://www.openmp.org/spec-html/5.0/openmpsu111.html#x148-6450003.2.2) returns the number of threads in the team 
  * [omp_get_thread_num()](https://www.openmp.org/spec-html/5.0/openmpsu113.html#x150-6570003.2.4) returns the unique id of the calling thread within the team (0, 1, ..., the number threads in the team - 1)
* You need `#include <omp.h>` to use these functions or any OpenMP API functions, for that matter
<!--- end md --->

<!--- md --->

#*P Using `omp_get_num_threads()` and `omp_get_thread_num()`

* Change following program so that each thread prints its id in the team and the number of threads in the team, like this.  The exact order of lines may differ.  Strictly speaking, even characters in two lines can be mixed into a single line.

```
hello
0/5 world
4/5 world
1/5 world
3/5 world
2/5 world
good bye
```
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_hello_id.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_hello.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_hello_id.c -o omp_hello_id
# nvc -mp omp_hello_id.c -o omp_hello_id
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_hello_id
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_hello_id_ans.c
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd01_omp/include/omp_hello.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_hello_id_ans.c -o omp_hello_id_ans
# nvc -mp omp_hello_id_ans.c -o omp_hello_id_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=3 ./omp_hello_id_ans
<!--- end code --->

<!--- md --->
# `#pragma omp for`

* `#pragma omp parallel` merely creates a team of threads executing the same statement
* In this sense, `#pragma omp parallel` alone cannot make a program run faster with multiple cores
* A program can be made faster only when you _divide_ the work among threads (work-sharing)
* `#pragma omp for` lets you divide iterations of a loop into threads created by `#pragma omp parallel`
<!--- end md --->

<!--- md --->
#*P How does `#pragma omp for` divide iterations to threads?

* Execute the following cell and observe which iteration is executed by which thread
* Based on the observation, change the number of iterations and threads and predict the mapping between iterations and threads
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_for.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_for.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_for.c -o omp_for
# nvc -mp omp_for.c -o omp_for
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=4 ./omp_for
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_

observation: in the default scheduling policy, which seems static, when executing with $P$ threads, the first $1/P$ of all iterations are executed by thread 0, next $1/P$ iterations by thread 1, etc.
<!--- end md --->


<!--- md --->
## for loops allowed by `#pragma omp for`

* There is a severe syntax restriction on the kind of for loops `#pragma omp for` can apply for
* See [Canonical Loop Form](https://www.openmp.org/spec-html/5.0/openmpsu40.html#x63-1260002.9.1) for the spec
* In short, it should look like `for (var = _init_; var < _limit_; var += _inc_)` where _init_, _limit_, and _inc_ are all loop-invariant (do not change throughout the loop)

## Combined pragma (parallel + for)

* `#pragma omp parallel` and `#pragma omp for` are often used together
* If `#pragma omp for` immediately follows `#pragma omp parallel`, they can be combined into a single pragma `#pragma omp parallel for`
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_parallel_for.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_parallel_for.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_parallel_for.c -o omp_parallel_for
# nvc -mp omp_parallel_for.c -o omp_parallel_for
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=4 ./omp_parallel_for
<!--- end code --->

<!--- md --->
# Scheduling a work-sharing for loop

* As you witnessed, the default scheduling policy in our environment (may be implementation dependent) seems static scheduling (assign roughly the same number of contiguous iterations to each thread)
* Is it enough? Clearly, it does not do a good job when iterations take a different amount of time
* You can change the policy by [schedule clause](https://www.openmp.org/spec-html/5.0/openmpsu41.html#x64-1290002.9.2)

## Visualizing scheduling

* The program below executes the function `iter_fun`
```
#pragma omp parallel for
  for (long i = 0; i < L; i++) {
    iter_fun(a, b, i, M, N, R, T);
  }
```

* `iter_fun(a, b, i, M, N, R, T)` repeats x = a x + b many (M * N) times and record time every N iterations
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_sched_rec.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_sched_rec.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp -D_GNU_SOURCE omp_sched_rec.c -o omp_sched_rec
# nvc -mp omp_sched_rec.c -o omp_sched_rec
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=4 ./omp_sched_rec > a.dat
<!--- end code --->

<!--- md --->
* Execute the following cell to visialize it
* In the graph,
  * horizontal axis is the time from the start in nanosecond
  * vertical axis is the iteration number
  * the color represents the thread that executed the iteration

<!--- end md --->

<!--- code w kernel=python points=1 --->
import sched_vis
sched_vis.sched_plt(["a.dat"])
# sched_vis.sched_plt(["a.dat"], start_t=1.5e7, end_t=2.0e7)
<!--- end code --->

<!--- md --->
#*P Understanding scheduling by visualization

* Add `schedule` clause to the program (`schedule(runtime)` allows you to set the schedule in the command line)
* Change the number of threads and schedule and observe how iterations are executed
* Set the number of threads very large (higher than the physical number of cores) and see what happens
  * Hint : you can get the number of cores by `nproc` command 
* In the above program, each iteration performs exactly the same amount of computation (i.e., x = a x + b (M * N) times), thus takes almost exactly the same time
* Specifically, make iteration `i` repeats x = a x + b (M * (i * N)) times (i.e., change the inner loop in `iter_fun` to `for (long k = 0; k < i * N; k++) { ...`)

* `sched_plt` function below takes optional parameters `start_t` and `end_t` specifying the horizontal range to display
* If you zoom _very_ closely to a particular point, you can see individual points and intervals between them, from which you can deduce how long it takes to perform `x = a x + b` once
<!--- end md --->

<!--- code w kernel=bash --->
nproc
<!--- end code --->

<!--- md --->
#*P Specifying the scheduling policy by schedule clause

* In the following (artificial) loop, iteration _i_ roughly sleeps for (100 x _i_) milliseconds and this is almost exactly the time it takes
1. predict the executing time of the parallel for loop with the default (static) scheduling policy
1. vary the scheduling policy and reason about their execution times

<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_schedule.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_schedule.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_schedule.c -o omp_schedule
# nvc -mp omp_schedule.c -o omp_schedule
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=4 ./omp_schedule
<!--- end code --->

<!--- md --->
* Predict the execution time with the default (static) policy
* Predict the execution time with the dynamic policy?
* Compare them with what you observed

* Explain your reasoning below
<!--- end md --->

<!--- md w points=1 --->

<!--- end md --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_

In this loop, later iterations take longer.  Therefore, 

* in the static policy, the execution time of the entire parallel loop will be the execution time of the thread 3, which are assigned iterations 9-11.  the execution time will therefore be ((9 + 10 + 11) * 100) milliseconds = 3.0 seconds

* in the dynamic policy, a crude (and optimistic) estimation, which assumes a perfect load balancing, is simply the serial execution time divided by the number of threads, which is ((0 + 1 + ... + 11) * 100) / 4 milliseconds = 1.65 seconds.

This crude approximation isn't quite acurate in this example, however.  To be more precise in this particular case, the likely scenario will be the following (a number represents the last digit of an iteration number).

```
[0] 444488888888
[1] 155555999999999
[2] 226666660000000000
[3] 333777777711111111111
```

In this case, the execution time will be (3 + 7 + 11) * 100 milliseconds = 2.1 seconds
<!--- end md --->

<!--- md --->
# Collapse clause

* `#pragma omp for` can specify a [collapse clause](https://www.openmp.org/spec-html/5.0/openmpsu41.html#x64-1290002.9.2) to apply work-sharing for a limited type of nested loops
* With a `#pragma omp for clause(2)`, OpenMP considers the doubly-nested loop that comes after this pragma the subject of work-sharing (i.e., distribute iterations of the doubly-nested loop to threads); you must have a _perfectly-nested_, rectangular doubly-nested loop after this clause
* A perfectly-nested loop is a nested loop whose outer loops (all loops except for the innermost one) do not have any statement except the inner loop. e.g.
```
for (i = 0; i < 100; i++) {
  for (j = 0; j < 100; j++) {
    S(i,j);
  }
}
```
is perfectly nested whereas
```
for (i = 0; i < 100; i++) {
  S;
  for (j = 0; j < 100; j++) {
    T;
  }
}
```
is not.  
* A perfectly nested loop is conceptually a flat loop with a mechanical transformation.
```
for (ij = 0; ij < 100 * 100; ij++) {
  i = ij / 100;
  j = ij % 100;
  S(i,j);
}
```
* A rectangular loop is a loop whose iteration counts of inner loops never depend on outer loops.  For example,
```
for (i = 0; i < 100; i++) {
  for (j = 0; j < i; j++) {
    S(i,j);
  }
}
```
is not a rectangular loop.
* Generally speaking, OpenMP `#pragma omp parallel` + `#pragma omp for` cannot handle nested parallelism very well, but collapse clause alleviates the problem to some extent
* Consider using tasks below for more general form of nested parallelism
<!--- end md --->

<!--- md --->
#*P Apply collapse and schedule

* Apply collapse and schedule to the following loop
* Trick: write `schedule(runtime)` and you can change the scheduling policy at execution time by setting environment variable `OMP_SCHEDULE=` in the command line. See [OMP_SCHEDULE environment variable](https://www.openmp.org/spec-html/5.0/openmpsu41.html#x64-1370002.9.2.1) for details
* Reason about the execution time of various schedule policies and with/without collapse
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_collapse.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_collapse.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_collapse.c -o omp_collapse
# nvc -mp omp_collapse.c -o omp_collapse
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_collapse
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_collapse_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_collapse.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_collapse_ans.c -o omp_collapse_ans
# nvc -mp omp_collapse_ans.c -o omp_collapse_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=3 ./omp_collapse_ans
<!--- end code --->

<!--- md --->
# Task parallelism

* Task is a more general mechanism to extract parallelism and distribute computation (called a task) _dynamically_ to threads in a team created by `#pragma omp parallel`
* A thread can create a task at any point in the execution of a parallel region and they are dispatched to available threads at runtime
* As a thread can create a task at any point, a task can create another task. that is, parallelism can be arbitrarily nested and the number of tasks can be difficult to predict (unlike the number of iterations of a for loop)
* A common pattern
  1. enter a parallel region by `#pragma omp parallel`
  1. ensure the statement is executed by only a single (root) thread with [#pragma omp master](https://www.openmp.org/spec-html/5.0/openmpse24.html#x118-4380002.16)
  1. create tasks at any point by [#pragma omp task](https://www.openmp.org/spec-html/5.0/openmpsu46.html#x70-2000002.10.1)
  1. a task waits for tasks it created to finish by [#pragma omp taskwait](https://www.openmp.org/spec-html/5.0/openmpsu93.html#x124-4690002.17.5)

* Let's see the effect of `#pragma omp master` first (without creating any task)
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_master.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_master.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_master.c -o omp_master
# nvc -mp omp_master.c -o omp_master
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_master
<!--- end code --->

<!--- md --->
* Since this is a common idiom, they can be combined into one pragma (`#pragma omp parallel master`)
  * <font color=red>This feature is not supported by NVIDIA compiler, however</font>
* The program below creates a parallel region whose entire region is executed only by the master and thus does not serve any useful purpose but mere a demonstration of the feature
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_parallel_master.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_parallel_master.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_parallel_master.c -o omp_parallel_master
# NVIDIA compiler does not support this program
# nvc -mp omp_parallel_master.c -o omp_parallel_master
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_parallel_master
<!--- end code --->

<!--- md --->
* Let's create a few tasks now
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_task.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_task.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_task.c -o omp_task
# nvc -mp omp_task.c -o omp_task
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_task
<!--- end code --->

<!--- md --->
* Tasks are particularly good at parallel recursions, as the following program demonstrates
* This is a common pattern that appears in many algorithms, particularly divide-and-conquer algorithms
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_rec_task.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_rec_task.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_rec_task.c -o omp_rec_task
# nvc -mp omp_rec_task.c -o omp_rec_task
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=10 ./omp_rec_task
<!--- end code --->

<!--- md --->
#*P A quiz about recursive tasks

* Answer the following questions
* How many tasks are created by `recursive_tasks(n, 0)`?  Include the caller of `recursive_tasks(n, 0)` as a task.  i.e., consider `recursive_tasks(0, 0)` creates one task
* How many of them are leaf tasks?
* Express them in terms of $n$
<!--- end md --->

<!--- md w points=1 --->
<!--- end md --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_

Let $T(n)$ be the number of times `recursive_tasks(n, 0)` encounters during the execution.  Then,

* $T(0) = 0$
* $T(n) = 2 + 2T(n - 1)$

Therefore, $T(n) = 2^{n+1} - 2$.  Counting the caller of `recursive_tasks(n, 0)` as a task, the answer is $(2^{n+1} - 1)$.

Let $L(n)$ be the number of leaf tasks created by `recursive_tasks(n, 0)`.  Then,

* $L(0) = 1$
* $L(n) = 2 L(n - 1)$

Therefore, the number of leaf tasks $= L(n) = 2^n$
<!--- end md --->

<!--- md --->
* Approximately what is the ideal execution time of `recursive_tasks(5, 0)` when using 10 threads?
* Compare it with what you observed
<!--- end md --->

<!--- md label=comment --->

* many think ideal execution time is 32 / 10 = 3.2 -> 3.2 x 0.3 = 0.96 and say the gap to 1.2 sec is overhead
* actually it is 4 x 0.3 = 1.2

<!--- end md --->

<!--- md w points=1 --->
<!--- end md --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_

Considering a leaf task takes 300 milliseconds, the time for non-leaf tasks will be negligible.
$2^5 = 32$ tasks will be distributed among 10 threads, so assuming a good load balancing, two of the ten threads will execute four leaf tasks and the other eight threads three leaf tasks.  Therefore execution time will be $300 \times 4$ milliseconds = 1.2 seconds.
<!--- end md --->

<!--- md --->
* Name an algorithm or two for which recursive tasks will be useful for parallelizing it and explain why you think so
<!--- end md --->

<!--- md label=comment --->

* many mention only one
* many say fibonnacci..., GCD, hanoi tower

<!--- end md --->


<!--- md w points=1 --->
<!--- end md --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_

* any algorithm for which recursive calls is useful and those recursive calls can be run in parallel will benefit from task parallelism. to name a few,

  * quicksort
  * mergesort
  * fast fourier transform
  * matrix matrix multiplication (formulated with divide-and-conquer for locality)
  * sparse matrix-vector multiplication (formulated with divide-and-conquer for locality)
  * $N$-body simulation
  * KD-tree construction

<!--- end md --->

<!--- md --->
# Taskloop

* As you can easily imagine, tasks can handle general nested loops if they can handle recursions
* Recent OpenMP actually has a construct just for that, which is [#pragma omp taskloop](https://www.openmp.org/spec-html/5.0/openmpsu47.html#x71-2080002.10.2)
  * <font color=red>This feature is not supported by NVIDIA compiler</font>
* Here is a demonstration showing it can handle non perfectly-nested loops
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_taskloop.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_taskloop.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_taskloop.c -o omp_taskloop
# nvc -mp omp_taskloop.c -o omp_taskloop
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=3 ./omp_taskloop
<!--- end code --->

<!--- md --->
<font color="red">NOTE:</font>

* Implementing task requires a more general mechanism than work-sharing for statement; the former should be able to distribute tasks generated in the course of execution, whereas the former merely needs to distribute iterations that are easily identifiable at the point of entering `#pragma omp for`, thanks to the ["canonical form" restriction](https://www.openmp.org/spec-html/5.0/openmpsu40.html#x63-1260002.9.1)
* Task scheduling is always dynamic whereas work-sharing for (particularly with static scheduling) gives you more control and predictability about which thread executes which iteration
* This is a reason why two mechanisms which are seemingly redundant exist, besides a historical reason that initially there was not a tasking construct in OpenMP
<!--- end md --->

<!--- md --->
# Data sharing

* OpenMP is a shared memory programming model, which means threads see updates among each other
* That is, when a thread updates a variable _x_ that is then read by another, the reader thread will see the updated value
* This is the default behavior of OpenMP ([Data Environment](https://www.openmp.org/spec-html/5.0/openmpse27.html#x135-5430002.19) in OpenMP spec)
* It is not always convenient, however
* `#pragma omp parallel` can thus specify whether local variables in the scope (i.e., defined outside the statement) are privatized (i.e., made private to each thread)
<!--- end md --->

<!--- md --->
#*P Observe the effect of privatization

* Execute the following and make sense of the output
* Add the `private(x)` clause and observe the difference
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_private.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_private.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_private.c -o omp_private
# nvc -mp omp_private.c -o omp_private
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=10 ./omp_private
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_private_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_private.c --->
<!--- end code --->

<!--- md --->
* `private(x)` essentially ignores the original variable `x` defined outside the parallel region and behaves as if a variable of the same name is defined by each thread
* `firstprivate(x)` is like `private(x)`, except that `x` of each thread is initialized by the value of `x` just before entering the parallel region
<!--- end md --->

<!--- md --->
#*P Observe the effect of `private` and `firstprivate`

* Execute the following and observe the output
* Add the `private(x)` clause and execute it
* Add the `firstprivate(x)` clause and execute it
* Make sense of the differences
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_firstprivate.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_firstprivate.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_firstprivate.c -o omp_firstprivate
# nvc -mp omp_firstprivate.c -o omp_firstprivate
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=3 ./omp_firstprivate
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code kernel=python label=ans --->
%%writefile omp_firstprivate_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_firstprivate.c --->
<!--- end code --->

<!--- code kernel=python label=ans --->
%%writefile omp_firstprivate_ans.c
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd01_omp/include/omp_firstprivate.c --->
<!--- end code --->

<!--- md --->
# Race condition

* Execute the above code without private or firstprivate many times
* Observe that the value of `x` after the parallel region is not always 123 + 5 (the number of threads executing the region), even different across runs
* For example, with two threads, the following execution order may cause such a behavior
  1. thread A reads 123
  1. thread B reads 123
  1. thread A writes 124
  1. thread B reads 124
* A similar case occurs whenever a thread's read-followed-by-write is intervened by another thread's update
* More generally, the following situation is called a "race condition" and if there is a race condition in your program, it almost always means your program is broken
  * Two or more threads concurrently access the same variable, and
  * at least one of them writes to it
Here, "concurrently access" means these accesses are not guaranteed to be separated in time by a synchronization primitive

* In all but trivial parallel programs, threads need to communicate with each other to accomplish a task
* Threads _communicate_ by having one thread write to a variable and having another read it
* If we simply do it without any mechanism to guarantee that they are separated in time, it is a race

* Below, we describe three ways to _safely_ communicate among threads without making race conditions

  * `#pragma omp critical`
  * `#pragma omp atomic`
  * reduction
<!--- end md --->

<!--- md --->
# `#pragma omp critical`

* [#pragma omp critical](https://www.openmp.org/spec-html/5.0/openmpsu89.html#x120-4470002.17.1) guarantees the statement following the pragma is executed not overlapping in time
<!--- end md --->

<!--- md --->
#*P Apply `#pragma omp critical`

* Execute the following program a few times and observe that the result is undeterministic and often not what we want (i.e., 123 + the number of threads)
* Then, add `#pragma omp critical` to the statement `x++` and see the result
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_critical.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_critical.c -o omp_critical
# nvc -mp omp_critical.c -o omp_critical
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=100 ./omp_critical
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_critical_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_critical_ans.c -o omp_critical_ans
# nvc -mp omp_critical_ans.c -o omp_critical_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=100 ./omp_critical_ans
<!--- end code --->

<!--- md --->
# `#pragma omp atomic`

* [#pragma omp atomic](https://www.openmp.org/spec-html/5.0/openmpsu95.html#x126-4840002.17.7) is similar to `#pragma omp critical` but its effect is slightly different and its applicability limited (see below)
<!--- end md --->

<!--- md --->
#*P Apply `#pragma omp atomic`

* Add `#pragma omp atomic` to the statement `x++` and see the result
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_atomic.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_atomic.c -o omp_atomic
# nvc -mp omp_atomic.c -o omp_atomic
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=100 ./omp_atomic
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_atomic_ans.c
<!--- exec-include ./mk_version.py -D VER=3 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_atomic_ans.c -o omp_atomic_ans
# nvc -mp omp_atomic_ans.c -o omp_atomic_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=100 ./omp_atomic_ans
<!--- end code --->

<!--- md --->
* The statement that follows this pragma cannot be an arbitrary expression
* See [atomic Construct](https://www.openmp.org/spec-html/5.0/openmpsu95.html#x126-4840002.17.7) for the spec
* Typically, it is an update to a variable, such as
```  
x += expr;
```  
* What is guaranteed by `#pragma omp atomic` is different from what `#pragma omp critical` guarantees
```
#pragma omp atomic
x += expr;
``` 
guarantees that the read and write to _x_ are never intervened by another update labeled `#pragma omp atomic` whereas
```
#pragma omp critical
x += expr;
``` 
guarantees that the entire statement `x += expr` does not overlap with another statement labeled critical.
* When applicable, `#pragma omp atomic` is more efficient than `#pragma omp critical` because the evaluation of expr can overlap
<!--- end md --->

<!--- md --->
# Reduction clause

* [Reduction](https://www.openmp.org/spec-html/5.0/openmpsu107.html#x140-5800002.19.5) is the best way to resolve race conditions where applicable and it often is
* It is applicable when threads altogether calculate $v = v_0 \oplus v_1 \oplus ... \oplus v_{n-1}$ where $v_i$ can be computed independently and $\oplus$ is an associative operator (such as +)
* In serial loop, this could be written by
```
v = initial value;
for (i = 0; i < n; i++) {
  v_i = ...
  v = v + v_i;
}
```
* If we parallelize the above loop, updating $v$ will result in a race condition
* This can be safely parallelized by introducing `reduction(+ : v)`
<!--- end md --->

<!--- md --->
#*P Apply reduction

* Add `reduction` clause to `#pragma omp parallel` below and observe the result
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_reduction.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash points=1 --->
clang -fopenmp omp_reduction.c -o omp_reduction
# nvc -mp omp_reduction.c -o omp_reduction
<!--- end code --->

<!--- code w kernel=bash points=1 --->
OMP_NUM_THREADS=100 ./omp_reduction
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_reduction_ans.c
<!--- exec-include ./mk_version.py -D VER=4 nb/source/pd01_omp/include/omp_race.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_reduction_ans.c -o omp_reduction_ans
# nvc -mp omp_reduction_ans.c -o omp_reduction_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=10 ./omp_reduction_ans
<!--- end code --->

<!--- md --->
# How reduction clause works and why it is preferable when applicable

* Where applicable, reduction is generally much faster than using `#pragma omp atomic` or `#pragma omp critical`
* This is because, internally, each thread computes its partial results using a private variable and combines their results only once at the end
* That is, each update, e.g., `x += expr` updates a thread's private version of variable x instead of updating the shared variable
* Omitting details you can think of what reduction clause is doing is to convert something like

```
int x = G;
#pragma omp parallel reduction(+ : x)
{
   ... x += expr; ...
}
  
```

into something like

```
int x = G;
#pragma omp parallel
{
  int x_priv = 0; // (I) initialize a private version of x
  {
   ... x_priv += expr; ...
  }
#pragma omp atomic
  x += x_priv; // (C) combine the partial results in the private variable into the global variable
}
```
* This is valid because of the associativity of the operation
<!--- end md --->

<!--- md --->
# User-defined reduction

* Reduction is a general concept for efficiently executing many computations of $v_i$'s in parallel, when the final outcome we wish to compute is $v = v_0 \oplus v_1 \oplus ... \oplus v_{n-1}$ 
* It is applicable whenever the order of combining partial results via $\oplus$ does not affect the final outcome (e.g., +)
* Yet the builtin reduction clause of OpenMP can only specify a few builtin operations for a few builtin types (e.g., int, float, etc.)
* You sometimes desire to apply the efficient execution mechanism of the reduction for more general types (perhaps types you defined)
* [User-defined reduction](https://www.openmp.org/spec-html/5.0/openmpsu107.html#x140-5800002.19.5) exists exactly for that
* You need to define an expression to
  * (C) combine two partial results into one (more specifically, combine a partial result assumed to be in a variable omp_in into another variable omp_out)
  * (I) initialize a thread-private version of the variable to which reduction is applied, named omp_priv
* For example, in the case of the builtin + operator, 
  * (C) would be omp_out += omp_in
  * (I) would be omp_priv = 0
<!--- end md --->

<!--- md --->
## Apply a user-defined reduction

* Here is a simple (broken) parallel for loop that is meant to do a reduction on 3-element vector
* Define a reduction with `#pragma omp declare reduction` and apply it to the parallel loop
<!--- end md --->

<!--- code w kernel=python --->
%%writefile omp_ud_reduction.c
<!--- exec-include ./mk_version.py -D VER=1 nb/source/pd01_omp/include/omp_ud_reduction.c --->
<!--- end code --->

<!--- code w kernel=bash --->
clang -fopenmp omp_ud_reduction.c -o omp_ud_reduction
# nvc -mp omp_ud_reduction.c -o omp_ud_reduction
<!--- end code --->

<!--- code w kernel=bash --->
OMP_NUM_THREADS=10 ./omp_ud_reduction
<!--- end code --->

<!--- code w kernel=python label=ans --->
%%writefile omp_ud_reduction_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_ud_reduction.c --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_ud_reduction_ans.c -o omp_ud_reduction_ans
# nvc -mp omp_ud_reduction_ans.c -o omp_ud_reduction_ans
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=10 ./omp_ud_reduction_ans
<!--- end code --->

<!--- md --->
#*P Putting them together: calculating an integral

Write an OpenMP program that calculates

$$ \int \int_D \sqrt{1 - x^2 - y^2}\,dx\,dy $$

where

$$ D = \{\;(x, y)\;|\;0\leq x \leq 1, 0\leq y \leq 1, x^2 + y^2 \leq 1 \}$$

* Note: an alternative way to put it is to calculate

$$ \int_0^1 \int_0^1 f(x)\,dx\,dy $$

where

$$ f(x) = \left\{\begin{array}{ll}\sqrt{1 - x^2 - y^2} & (x^2 + y^2 \leq 1) \\ 0 & (\mbox{otherwise}) \end{array}\right. $$

* Use a nested loop to calculate the double integral
* Try work-sharing for, taskloop, recursive tasks to parallelize it
* The result should be close to $\pi/6 = 0.52359..$ (1/8 of the volume of the unit ball)
* Play with the number of infinitesimal intervals for integration and the number of threads so that you can observe a speedup
* As you are using a shared cloud environment, you do not have to be serious about speedup (nearly perfect speedup is unlikely when other students are simultaneously using the same machine and/or the cloud is doing many other stuff (e.g., servicing the page you are looking at right now)

* If you want to work with an editor you are accustomed to rather than web browser, see [this page](https://taura.github.io/programming-languages/html/jupyter.html?lang=en)
<!--- end md --->

<!--- code w kernel=python points=1 --->
%%writefile omp_integral.c

<!--- end code --->

<!--- md label=comment --->
* N を指定しないと採点がしづらい
* 台数効果を測るように指示しないと採点しづらい
<!--- end md --->

<!--- md label=ans --->
_<font color="green">Example answer:</font>_
<!--- end md --->
<!--- code w kernel=python label=ans --->
%%writefile omp_integral_ans.c
<!--- exec-include ./mk_version.py -D VER=2 nb/source/pd01_omp/include/omp_integral.c --->
<!--- end code --->

<!--- md label=ans --->
_<font color="green">Remarks:</font>_

* The first question is whether or not you use `collapse` to parallelize the inner loop
* If you do, using the break statement in the inner loop becomes invalid, so you cannot break the inner loop even when $(1 - x * x - y * y)$ becomes $< 0$
* If you do not, it is safe to do so, and in fact you want to do so as soon as $(1 - x * x - y * y)$ becomes $< 0$, to avoid wasting time calculating $(1 - x * x - y * y)$ in later iterations when you know they are all negative
* If the outer loop has enough iterations ($\gg$ the number of threads), then the latter is much more efficient

* Either case, the right scheduling strategy will be dynamic, as the work per iteration is not uniform.  If you do collapse, the work per (inner) iteration differs depending on whether you perform `sqrt` or not.  If you do not collapse, the work per (outer) iteration differs more significantly depending on where you break the loop.
* If you do collapse, the pitfall is that the work per iteration becomes so small that using the default chunk size (1) makes the dynamic load balancing overhead too large to enjoy any benefit of parallelizaiton. Make the chunk size larger to amortize the overhead. If you do not collapse, a single iteration executes an entire inner loop, so the overhead of dynamic load balancing is hardly an issue in practice.
<!--- end md --->


<!--- md --->
* Compile it
<!--- end md --->

<!--- code w kernel=bash points=1 --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
clang -fopenmp omp_integral_ans.c -o omp_integral_ans
# nvc -mp omp_integral.c -o omp_integral
<!--- end code --->

<!--- md --->
* and run it
<!--- end md --->

<!--- code w kernel=bash points=1 --->
<!--- end code --->

<!--- code w kernel=bash label=ans --->
OMP_NUM_THREADS=8 ./omp_integral_ans
<!--- end code --->



