""" md
#* The cost of data accesses --- caches and prefetches

"""



""" md
# Measuring latency

* let's measure the latency of data access by a load instruction, which depends on _where_ the data comes from (L1 cache, L2 cache, L3 cache, and the main memory)
* we do so by _pointer chasing_, which is essentially the following loop
```
for many times {
  p = p->next;
}
* we expect this code to take (the latency of the load instruction for p->next) $\times$ the number of iterations
```
* for a reason not very relevant here, the following program actually does it by 
```
repeat {
  k = a[k]
}
```
(the difference is not important here. both end up doing almost the same thing)
* the point here is data access (`a[k]`) needs the previous access to have been finished, as it uses `k` _to determine the address accessed_ and `k` is generated by the previous access

"""

""" code w kernel=python """
%matplotlib widget

# trick to make the visualization nicer
""" """

""" code w kernel=python """
%%writefile ptr_chase.cc
""" exec-include ./mk_version.py -D VER=1 nb/source/pd07_mem/include/mem.cc """
""" """

""" md 

* the main function including command line argument processing, setting up the array, and measuring cycles and other things 
* by default, it measures 
  * the number of instructions executed (instructions)
  * the number of core cycles (cycles)
  * L1 cache misses (L1-dcache-load-misses)
  * what is apparently L3 cache misses (cache-misses, with a caveat)

* in this environment, I don't know how to measure L2 cache misses
* `perf list` command shows everything we can measure, but there are no event names that specifically correspond to L2 cache misses
"""

""" code w kernel=python """
%%writefile mem_main_cpu.cc
""" exec-include ./mk_version.py -D VER=1 nb/source/pd07_mem/include/mem_main.cc """
""" """

""" md 

* compile (either by g++, clang++, or nvc++, whichever you like)

* note: we are not using bash kernel in SoS notebook but python kernel, for the purpose of better visualization of the graph
  * `!` is the trick to execute a shell command within a python kernel

"""

""" code w kernel=python """
! g++                          -Wall -O3 -mavx512f mem_main_cpu.cc ptr_chase.cc -o ptr_chase -lpfm
! # /home/share/llvm/bin/clang++ -Wall -O3 -mavx512f mem_main_cpu.cc ptr_chase.cc -o ptr_chase -lpfm
! # /opt/nvidia/hpc_sdk/Linux_x86_64/22.11/compilers/bin/nvc++ -Wall -O3 -mavx512f mem_main_cpu.cc ptr_chase.cc -o ptr_chase -lpfm
""" """

""" md 

* run for help

"""

""" code w kernel=python """
! ./ptr_chase --help
""" """

""" md 

```
./ptr_chase -n N -s S -a A
```
creates an array of $N$ bytes and accesses it with $S$-byte stride $A$ times.

* the command below creates an array of 16KB ($<$ L1 cache size) accesses elements 100M times with 64-byte stride ($=$ cache line size)

"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 64 -a $((100 * 1000 * 1000))
""" """

""" md

* observe
   * the number of instructions
   * the number of cycles (core cycles)
and calculate the latency of a single data access (`a[k]`)

* witness the assembly code, generated by g++ and/or clang++ to make sense of the number of instructions and the latency

"""

""" code w kernel=python """
! g++                          -Wall -O3 -mavx512f ptr_chase.cc -S
! # /home/share/llvm/bin/clang++ -Wall -O3 -mavx512f ptr_chase.cc -S
! # /opt/nvidia/hpc_sdk/Linux_x86_64/22.11/compilers/bin/nvc++ -Wall -O3 -mavx512f ptr_chase.cc -S
""" """





""" md 

##* A few notes on how to make that happen

* it creates an array of `long` having size of $N$ bytes ($N$ is rounded up to the next multiple of `sizeof(long)`; e.g., $N=12345$ -&gt; an array of 12352 bytes = 1544 `long` elements will be created)
* initialize the array (let's call it$a$) so that elements are accessed with $S$-byte stride. Again, $S$ is rounded up to the next multiple of `sizeof(long)`; e.g., $S=123$ -&gt; elements are accessed with 128-byte stride, which is 16-element stride.  That is, $a[k] = k + 16$.  To accommodate the case where $(k + 16)$ overflows the array, it is actually initialized with $a[k] = (k + 16) \mbox{ (mod the number of elements of the array)}$.
* then it enters the loop
```
k = 0;
for A times {
  k = a[k];
}
```

* As a specific example, let's say $N=576$ ($=$ 72 elements), $S=128$ ($=$ 16 elements), then the above loop will access

```
a[0], a[16], a[32], a[48], a[64], a[8], a[24], a[40], a[56], a[0], a[16], a[32], a[48], a[64], a[8], a[24], a[40], a[56], ... (repeats) ...
```

* since accessing an element (say a[0]) will bring the entire 64-byte block containing it into the cache and our primary interest in cache behavior, we set parameters so that we access only a single element in each 64-byte block.  
* to that end, we typically specify $S$ be a multiple of 64
* for a similar reason $N$ is also typically a multiple of 64
* if a largest common divisor of $N$ and $S$ is 64, then the loop will touch every 64-byte block of the array (as it did in the above example)
* if they have a larger common divisor (e.g., 128), there are 64-byte blocks of the array that are not accessed at all by the above loop 
* for example, when $N=512$ and $S=128$ (GCD($N$, $S$) = 128), then the loop accesses `a[0], a[16], a[32], a[48], a[0], a[16], a[32], a[48], ...(repeats)...` leaving the 64-byte block containing `a[8], a[24], a[40], a[56]` not accessed
* to help you understand what's going on, the program shows the number of distinct 64-byte blocks accessed in the line `distinct blocks`

"""

""" md
* will touch all blocks
"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 64 -a $((100 * 1000 * 1000))
""" """

""" md
* will touch only half of the blocks (GCD(16 * 1024, 128) = 128)
"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 128 -a $((100 * 1000 * 1000))
""" """

""" md
* will touch only a forth of the blocks (GCD(16 * 1024, 256) = 256)
"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 256 -a $((100 * 1000 * 1000))
""" """

""" md
* will touch all blocks despite S=256, as GCD(16 * 1024 + 64, 256) = 64
"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024 + 64)) -s 256 -a $((100 * 1000 * 1000))
""" """

""" md 

* if you cyclically accesses the set of data that do not fit a level of cache (e.g., L1 cache), then you will observe the cost of data access going up
* 
* change the argument given to `-n` (16 * 1024) in the following and see what happens on metrics reported (instructions, cycles, cache misses)

"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 64 -a $((100 * 1000 * 1000))
""" """

""" md 

#*P Find out the size and latency of caches/memory

* the latency per access we observed above indicates L1 cache latency (as the data size is smaller than L1 cache)
* by increasing the data size (`-n`), find out the data size around which the latency as well as the number of L1 cache misses sharply increase from the above experiment
* it indicates the L1 cache size and the latency of the L2 cache
* you may increase the data size further to find out L2 cache size and L3 cache latency, and L3 cache size and the main memory latency
* to the extend possible try to find them out
 
* you may be able to Google cache sizes of the processor we are using (Intel(R) Xeon(R) Platinum 8368 CPU @ 2.40GH) but are encouraged to _observe_ them by yourself
* also, get a sense of latency of each memory hierarchy

"""

""" code w kernel=python """
! ./ptr_chase -n $((16 * 1024)) -s 64 -a $((100 * 1000 * 1000))
""" """

""" md w points=1

|  |size|latency|
|--|----|-------|
|L1|    |       |
|L2|    |       |
|L3|    |       |
|main memory|X|       |


"""

""" md 

# Visualizing latency

* run the program with many data sizes and visualize the latency across the whole range
* here is a helper program that runs the program with many data sizes and strides

"""

""" code w kernel=python """
""" include nb/source/pd07_mem/include/run.py """
""" """

""" md

* `seq(a, b, r, u, 0)` generates a geometric sequence of the ratio $r$ starting near $a$ and ending near $b$
* each number is made a multiple of $u$
* `seq(a, b, r, u, 1)` is similar to `seq(a, b, r, u, 0)`, except each number is made a _prime_ multiple of $u$

"""

""" code w kernel=python """
seq(2 ** 10, 2 ** 20, 1.3, 64, 1)
""" """

""" md

* we like to test the whole range of data sizes from sizes smaller than L1 cache (a few K bytes) to those much larger than L3 cache (several hundreds of M bytes)

* we make the stride large (4096) for a reason described later
* the purpose is to make it look like random accesses from the processor's point of view
* as the sizes are generated as a prime multiple of 64, GCD(size, stride) is always 64, ensuring all 64-byte blocks are accessed

"""

""" code w kernel=python """
run("./ptr_chase",
    # generate a geometric sequence from 2^10 to 2^28
    seq(2 ** 10, 2 ** 28, 1.3, 64, 1),
    # make stride always 64
    [ 4096 ])
""" """

""" md

* data are dumped into many files in out/out_*.txt

"""

""" code w kernel=python """
! ls out/
""" """

""" md

* data reader

"""

""" code w kernel=python """
""" include nb/source/pd07_mem/include/read_data.py """
""" """

""" md

* visualizer for size vs latency

"""

""" code w kernel=python """
""" include nb/source/pd07_mem/include/size_vs_lat.py """
""" """

""" md

* let's visualize it

"""

""" code w kernel=python """
size_vs_what(glob.glob("out/out_*.txt"))
""" """

""" md

* from the visualization, find again the size and latency of each level of the caches and the latency of main memory

"""

""" md

* the same program can visualize cache misses (measured by Linux perf, which uses hardware counters to count cache misses)

"""

""" code w kernel=python """
size_vs_what(glob.glob("out/out_*.txt"), what="L1-dcache-load-misses")
size_vs_what(glob.glob("out/out_*.txt"), what="cache-misses")
""" """

""" md 

# Prefetching

* run the same experiment again, but this time with a small stride (64)

"""

""" code w kernel=python """
run("./ptr_chase",
    # generate a geometric sequence from 2^10 to 2^28
    seq(2 ** 10, 2 ** 28, 1.3, 64, 1),
    # make stride always 64
    [ 64 ])
""" """

""" md

* let's visualize it

"""

""" code w kernel=python """
size_vs_what(glob.glob("out/out_*.txt"))
""" """

""" md

* why is the latency of the case stride=64 much smaller?
* it's not that individual accesses literally take shorter from the memory (or whichever cache level the data happens to be in) to the processor, but that the processor detected sequential access and fetch data earlier than the corresponding load instruction is actually issued
* it happens only within the same OS page (4KB), so with stride $\geq 4096$, the effect of prefetching becomes none and even for strides $< 4096$, the effect of prefetching decreases as the next data is more likely to be on a different page
* if you like, run the experiments with various strides (give a list of strides you want to use as the third parameters of run, like run(..., ..., [256, 1024, 16384])

"""

""" code w kernel=python """
size_vs_what(glob.glob("out/out_*.txt"), what="L1-dcache-load-misses")
size_vs_what(glob.glob("out/out_*.txt"), what="cache-misses")
""" """

""" md 

# Translating it to bandwidth

* visualize the same result with a bandwidth perspective
* it is simply the number of data accessed in total divided by the number of cycles
  * the former is 64 $\times$ the number of iterations (given as `-a`).  64 is the cache line size. we count a single element access as 64 bytes even if we actually touch only 8 bytes of it, because the processor brings the entire cache line anyways

"""

""" code w kernel=python """
""" include nb/source/pd07_mem/include/size_vs_bw.py """
""" """

""" code w kernel=python """
size_vs_bw(glob.glob("out/out_*.txt"))
size_vs_bw(glob.glob("out/out_*.txt"))
""" """

""" md

* recall how much was enough to get close to processor's peak performance for SpMV, if we (conservatively) assume a single fma takes accessing 8 bytes (a single DP element, only counting the matrix element)
* as the processor's peak is 16 DP fmas/cycle, it needs to bring 8 x 16 = 128 bytes/cycle to the processor
* it is far from the numbers we are seeing, even if data are coming from L1 cache
* if you multiply the processor's frequency (2.4GHz) to the bytes/cycle, you get the bandwidth in terms of bytes/sec
  * equivalently you multiply bytes/cycle by 2.4 to get GB/sec
* observe that the main memory bandwidth, even with the small stride (64) where prefetching is going on, it is just a few GB/sec, far from what the processor's spec sheet advertises

"""

""" md 

# Why you'd better avoid large-power-of-two strides?

* a data element is deterministically mapped to a particular _set_ in the cache and it is the data in each set that are managed by LRU
* if data you are actively using are mapped on the same set, then the cache miss sharply increase, even if the total size of those data is much smaller than the cache (conflict misses)
* in the case of 12-way set associative, 48KB L1 cache, data whose addresses are apart by a multiple of 4KB (48KB/12) are mapped on the same cache set, therefore if you cyclically access (12 + 1) elements with 4KB stride, it induces almost 100% cache misses, even if the total data size actually used is just 64 x 13 = 832 bytes
  
* contrast the following two
* observe the "distinct_blocks" reported
"""

""" code w kernel=python """
! ./ptr_chase -n $((4096 * 11)) -s 4096 -a $((100 * 1000 * 1000))
! ./ptr_chase -n $((4096 * 13)) -s 4096 -a $((100 * 1000 * 1000))
""" """

""" md 

* stride accesses happen most typically for multidimensional arrays
* if, for example, you have 2D array (matrix) of DP numbers that has 1024 elements in a row, two adjacent elements in a column are 8192 bytes apart in the row-major order
* therefore scanning elements in the column direction (i.e., `a(i,j), a(i+1,j), a(i+2,j), ...`) would access them in a large-power-of-two stride
* this access pattern appears in matrix-matrix multiplication for matrix B (of A * B)
* to avoid this effect, it's good idea to adjust the number of elements in a row by padding (make the matrix a bit fatter than what it actually is, to make the address distance between two adjacent elements in a column not a large power of two). 64 $\times$ an odd number is a safe choice
"""

